{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "**Run on Google Colab (Quickstart)**\n",
    "\n",
    "```bash\n",
    "! git clone --branch main --single-branch https://github.com/sbaaihamza/scrapping-lib.git\n",
    "%cd scrapping-lib\n",
    "! pip install -e \".[browser,dev]\"\n",
    "! playwright install\n",
    "# Preferred (installs OS deps automatically on supported distros):\n",
    "! playwright install --with-deps chromium\n",
    "# If needed (manual deps fallback):\n",
    "! apt-get update\n",
    "! apt-get install -y libxcomposite1 libxcursor1 libgtk-3-0 libatk1.0-0 libcairo2 libgdk-pixbuf2.0-0\n",
    "%cd /content/scrapping-lib/notebooks\n",
    "```\n",
    "\n",
    "*Note: Playwright has both sync and async APIs. These notebooks are designed to be async-safe for Jupyter/Colab. If you encounter OS dependency issues, use the `playwright install --with-deps chromium` command.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c882580",
   "metadata": {},
   "source": [
    "# Online Scraping Playbook: Building & Adjusting Configs\n",
    "\n",
    "This notebook teaches you how to build and refine scraping configurations for new websites using the `scrapping` library. It focuses on real-world challenges like engine selection, link extraction, and quality control.\n",
    "\n",
    "## Purpose\n",
    "- Learn the step-by-step workflow for onboarding a new site.\n",
    "- Understand how to diagnose and handle common scraping obstacles responsibly.\n",
    "- Master the configuration schema to balance speed and accuracy.\n",
    "\n",
    "## Responsible Scraping: Do's and Don'ts\n",
    "Compliance and ethics are central to our scraping practice.\n",
    "\n",
    "### DO:\n",
    "- **Check `robots.txt` and Terms of Service**: Always respect the site's guidelines.\n",
    "- **Use APIs first**: If a site provides a legitimate API, prefer it over HTML scraping.\n",
    "- **Rate limit**: Be a good citizen. Don't hammer servers; add delays and limit concurrency.\n",
    "- **Obtain permission**: For large-scale data collection, reach out to the site owner when in doubt.\n",
    "\n",
    "### DON'T:\n",
    "- **Bypass CAPTCHAs**: We do not include evasion logic. If blocked by a CAPTCHA, stop and seek an alternative path.\n",
    "- **Circumvent Access Controls**: Do not attempt to bypass login walls or restricted areas without proper authorization.\n",
    "- **Ignore Rate Limits**: Bypassing 429 errors by cycling IPs aggressively is against our policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32c658",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Initialize the environment and detect the repository root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c030155",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nimport sys\nfrom pathlib import Path\n\nfrom scrapping.engines.http import HttpEngine\nfrom scrapping.extraction.link_extractors import LinkExtractRequest, extract_links\nfrom scrapping.orchestrator import Orchestrator, OrchestratorOptions, validate_config\nfrom scrapping.processing.html_to_structured import html_to_structured\nfrom scrapping.processing.quality_filters import evaluate_quality\n\n\ndef find_repo_root(start_path):\n    p = Path(start_path).resolve()\n    for parent in [p] + list(p.parents):\n        if (parent / 'pyproject.toml').exists():\n            return parent\n    return p\n\nREPO_ROOT = find_repo_root(Path.cwd())\nsys.path.append(str(REPO_ROOT))\nos.chdir(str(REPO_ROOT))\n\nONLINE = os.getenv('ONLINE', '0') == '1'\nRESULTS_DIR = Path('results_notebook_online')\nprint(f'Python version: {sys.version}')\nprint(f'Repo root: {REPO_ROOT}')\nprint(f'Online mode: {ONLINE}')\nprint(f'Results will be saved in: {RESULTS_DIR}')"
  },
  {
   "cell_type": "markdown",
   "id": "5acd9806",
   "metadata": {},
   "source": [
    "## 2. Config Anatomy Refresher\n",
    "A typical source config consists of several key sections that define the 'what', 'how', and 'where' of a scraping task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05654ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_config = {\n",
    "    \"source_id\": \"example_site\",\n",
    "    \"engine\": { \n",
    "        \"type\": \"http\",           # Choose: http, browser, hybrid\n",
    "        \"timeout_s\": 15,          # Max time per request\n",
    "        \"verify_ssl\": True        # Safety first\n",
    "    },\n",
    "    \"entrypoints\": [ \n",
    "        { \"url\": \"https://example.com/items?page={page}\", \"paging\": {\"mode\": \"page\", \"start\": 1, \"end\": 3} }\n",
    "    ],\n",
    "    \"discovery\": {\n",
    "        \"link_extract\": {\n",
    "            \"method\": \"regex\",\n",
    "            \"pattern\": \"https://example\\.com/items/\\d+\"\n",
    "        }\n",
    "    },\n",
    "    \"quality\": {\n",
    "        \"min_text_len\": 200\n",
    "    }\n",
    "}\n",
    "print(json.dumps(minimal_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433cac4",
   "metadata": {},
   "source": [
    "## 3. Website Triage Checklist\n",
    "Choosing the right engine is crucial for performance and reliability.\n",
    "\n",
    "### Decision Guide:\n",
    "1. **Use HTTP** when:\n",
    "   - Server renders HTML (check `view-source` in your browser).\n",
    "   - Links and text are clearly visible in the raw response.\n",
    "   - Site is fast and handles high-volume requests well.\n",
    "2. **Use Browser** when:\n",
    "   - Content is loaded via JavaScript after the initial page load.\n",
    "   - The site is a Single Page Application (SPA).\n",
    "   - You need to interact with the page (scrolling, clicking) to reveal content.\n",
    "3. **Use Hybrid** when:\n",
    "   - The listing pages are fast and static (HTTP is fine).\n",
    "   - Individual detail pages require JS rendering (Browser is needed).\n",
    "\n",
    "### Signals to Check:\n",
    "- **Raw Source**: Press `Ctrl+U`. If the text you want isn't there, you probably need `browser` engine.\n",
    "- **Network Tab**: Press `F12` and check the Network tab. If you see JSON responses with your data, you might be able to target an API directly via HTTP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7d3de",
   "metadata": {},
   "source": [
    "## 4. Online Debug Harness\n",
    "These helpers allow us to quickly test site responses and detect blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_fetch_http(url):\n",
    "    engine = HttpEngine()\n",
    "    res = engine.get(url)\n",
    "    print(f'Status: {res.status_code}')\n",
    "    print(f'Length: {len(res.text) if res.text else 0}')\n",
    "    if res.text:\n",
    "        print(f'Snippet: {res.text[:500]}...')\n",
    "    return res\n",
    "\n",
    "def detect_blocking(fetch_result):\n",
    "    if not fetch_result.ok:\n",
    "        if fetch_result.status_code in (403, 429):\n",
    "            return 'likely_blocked'\n",
    "        return 'failed_request'\n",
    "    \n",
    "    text = (fetch_result.text or '').lower()\n",
    "    blocked_patterns = ['captcha', 'cloudflare', 'unusual traffic', 'access denied', 'forbidden']\n",
    "    for p in blocked_patterns:\n",
    "        if p in text:\n",
    "            return 'likely_blocked'\n",
    "    \n",
    "    auth_patterns = ['login required', 'sign in to continue', 'please log in']\n",
    "    for p in auth_patterns:\n",
    "        if p in text:\n",
    "            return 'requires_auth'\n",
    "            \n",
    "    return 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8151815",
   "metadata": {},
   "source": [
    "## 5. Step-by-Step: Build a Config for a New Site\n",
    "Let's walk through creating a configuration for a hypothetical site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11599b21",
   "metadata": {},
   "source": [
    "### Step A & B: Template and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db245200",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id = 'my_new_site'\n",
    "entrypoint = 'https://example.com/listings' # Replace with real if ONLINE=1\n",
    "\n",
    "new_source = {\n",
    "    'source_id': source_id,\n",
    "    'engine': {'type': 'http'},\n",
    "    'entrypoints': [{'url': entrypoint}],\n",
    "    'storage': {'items_format': 'jsonl'}\n",
    "}\n",
    "\n",
    "v = validate_config({'sources': [new_source]})\n",
    "print(f'Validation ok: {v[\"ok\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed8312",
   "metadata": {},
   "source": [
    "### Step C: Fetch & Inspect Listing\n",
    "We extract links to find our detail pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f585900",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ONLINE:\n",
    "    print(f'Fetching: {entrypoint}')\n",
    "    res = debug_fetch_http(entrypoint)\n",
    "    html = res.text\n",
    "else:\n",
    "    print('ONLINE=0: using fixtures (naukrigulf)')\n",
    "    with open('tests/fixtures/html/listing_naukrigulf.html') as f:\n",
    "        html = f.read()\n",
    "\n",
    "# Define extraction (adjust as needed based on inspection)\n",
    "link_pattern = r'https://www\\.naukrigulf\\.com/.*-jobs-\\d+'\n",
    "req = LinkExtractRequest(html=html, method='regex', pattern=link_pattern)\n",
    "links = extract_links(req)\n",
    "print(f'Found {len(links)} links. Top 3: {links[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d8ca1",
   "metadata": {},
   "source": [
    "### Step D, E & F: Parse Detail and Quality\n",
    "We take one link, fetch it, and extract structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_url = links[0] if links else 'https://example.com/detail/1'\n",
    "if ONLINE:\n",
    "    print(f'Fetching detail: {detail_url}')\n",
    "    res = debug_fetch_http(detail_url)\n",
    "    detail_html = res.text\n",
    "else:\n",
    "    print('ONLINE=0: using fixtures (naukrigulf)')\n",
    "    with open('tests/fixtures/html/detail_naukrigulf.html') as f:\n",
    "        detail_html = f.read()\n",
    "\n",
    "doc = html_to_structured(detail_html, url=detail_url)\n",
    "item = doc.as_item()\n",
    "print(f'Extracted Title: {item[\"title\"]}')\n",
    "\n",
    "q = evaluate_quality(item, rules={'min_text_len': 300})\n",
    "print(f'Quality Check: Keep={q.keep}, Issues={[i.code for i in q.issues]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125d0fd",
   "metadata": {},
   "source": [
    "### Step G & H: Paging and Actions\n",
    "Configure how to handle multiple pages and interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paging_example = {\n",
    "    'mode': 'page',\n",
    "    'start': 1,\n",
    "    'end': 10,\n",
    "    'page_param': 'p' # e.g. ?p=1, ?p=2\n",
    "}\n",
    "\n",
    "actions_example = [\n",
    "    {'type': 'scroll', 'params': {'mode': 'down', 'repeat': 3}},\n",
    "    {'type': 'wait_for', 'selector': '.content-loaded'}\n",
    "]\n",
    "print('Paging and Actions are set in the source config to handle dynamic loading.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932eac75",
   "metadata": {},
   "source": [
    "### Step I: Small Online Trial\n",
    "Run the full orchestrator for a limited scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28fafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ONLINE:\n",
    "    trial_cfg = {'sources': [new_source]}\n",
    "    # Restrict to 1 page for trial\n",
    "    trial_cfg['sources'][0]['entrypoints'][0]['paging'] = {'pages': 1}\n",
    "    \n",
    "    orch = Orchestrator(options=OrchestratorOptions(results_dir=RESULTS_DIR))\n",
    "    out = orch.run(trial_cfg)\n",
    "    print(f'Trial complete. Summary: {out[\"summary\"]}')\n",
    "else:\n",
    "    print('ONLINE=0: skipping trial.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de152e7d",
   "metadata": {},
   "source": [
    "## 6. Real-World Multi-Source Example\n",
    "Applying the logic to `examples/configs/example_multi_sources.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('examples/configs/example_multi_sources.json') as f:\n",
    "    multi_cfg = json.load(f)\n",
    "\n",
    "for s in multi_cfg['sources']:\n",
    "    sid = s['source_id']\n",
    "    eng_type = s.get('engine', {}).get('type')\n",
    "    print(f'Source: {sid:<15} | Engine: {eng_type}')\n",
    "    \n",
    "    if ONLINE:\n",
    "        # Tiny trial run\n",
    "        sub_cfg = {'sources': [s]}\n",
    "        orch = Orchestrator(options=OrchestratorOptions(results_dir=RESULTS_DIR / 'multi_trial'))\n",
    "        res = orch.run(sub_cfg)\n",
    "        print(f'  -> Run Result: {res[\"summary\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f337f",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting Cookbook (Safe)\n",
    "| Symptom | Likely Cause | Safe Fix |\n",
    "| :--- | :--- | :--- |\n",
    "| **403 Forbidden / 429 Too Many Requests** | Rate limiting / Bot detection | Reduce RPS (rps/burst), increase `min_delay_s`, or use backoff retry policy. |\n",
    "| **Empty HTML / No links found** | Client-side rendering (JS) | Switch to `browser` or `hybrid` engine and add `wait_for` selectors. |\n",
    "| **'Login Required' messages** | Authenticated session needed | Use a legitimate API or provide an authenticated session token you already possess. |\n",
    "| **CAPTCHA appears** | Suspicious traffic detected | **Stop scraping**. Use a permitted access path or API (No bypass logic permitted). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7ba15",
   "metadata": {},
   "source": [
    "## 8. From Config to Regression Tests\n",
    "Once a config is working, lock it in with a test using snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d16384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_test(source_id, listing_html, detail_html):\n",
    "    # Pseudo-code for creating a test\n",
    "    print(f'1. Saving listing snapshot: tests/fixtures/html/listing_{source_id}.html')\n",
    "    print(f'2. Saving detail snapshot:  tests/fixtures/html/detail_{source_id}.html')\n",
    "    print(f'3. Adding assertion: assert len(extract_links(...)) == {len(links)}')\n",
    "\n",
    "create_regression_test('my_new_site', '...', '...')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
