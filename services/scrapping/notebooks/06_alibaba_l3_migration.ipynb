{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run on Google Colab (Quickstart)**\n",
    "\n",
    "```bash\n",
    "! git clone --branch main --single-branch https://github.com/sbaaihamza/scrapping-lib.git\n",
    "%cd scrapping-lib\n",
    "! pip install -e \".[browser,dev]\"\n",
    "! playwright install\n",
    "# Preferred (installs OS deps automatically on supported distros):\n",
    "! playwright install --with-deps chromium\n",
    "# If needed (manual deps fallback):\n",
    "! apt-get update\n",
    "! apt-get install -y libxcomposite1 libxcursor1 libgtk-3-0 libatk1.0-0 libcairo2 libgdk-pixbuf2.0-0\n",
    "%cd /content/scrapping-lib/notebooks\n",
    "```\n",
    "\n",
    "*Note: Playwright has both sync and async APIs. These notebooks are designed to be async-safe for Jupyter/Colab. If you encounter OS dependency issues, use the `playwright install --with-deps chromium` command.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Alibaba Scraper: Hard Target Workshop\n",
    "\n",
    "This notebook is a workshop-style guide to building a production-grade scraper for Alibaba. We follow the **Methodology Ladder**: starting simple, diagnosing failures, and escalating to browser rendering and the Recipe framework.\n",
    "\n",
    "## 0) Readiness Check\n",
    "Before we start, we ensure the environment has all required OS dependencies for the Browser engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "readiness-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.orchestrator import doctor_environment\n",
    "doctor = doctor_environment()\n",
    "pw_check = doctor['checks'].get('playwright_browsers', {})\n",
    "if pw_check.get('ok'):\n",
    "    print(\"‚úÖ Playwright is ready.\")\n",
    "else:\n",
    "    print(f\"‚ùå Playwright Issue: {pw_check.get('msg')}\")\n",
    "    if 'hint' in pw_check:\n",
    "        print(f\"üëâ Hint: {pw_check['hint']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## 1) Step 1: Minimal Config & HTTP Attempt\n",
    "\n",
    "We start with the simplest engine (HTTP) to see if the content is static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step1-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scrapping.engines.http import HttpEngine\n",
    "from scrapping.diagnostics.classifiers import diagnose_http_response, recommend_next_step\n",
    "\n",
    "ONLINE = os.getenv('ONLINE', '0') == '1'\n",
    "url = \"https://www.alibaba.com/trade/search?SearchText=mechanical+keyboard\"\n",
    "\n",
    "if ONLINE:\n",
    "    engine = HttpEngine()\n",
    "    res = engine.get(url)\n",
    "    diag = diagnose_http_response(res.status_code, res.response_meta.headers, res.text)\n",
    "    print(f\"HTTP Status: {res.status_code} | Text Len: {len(res.text or '')}\")\n",
    "    print(f\"Diagnosis: {diag.label.value} | Rec: {recommend_next_step(diag)}\")\n",
    "else:\n",
    "    print(\"OFFLINE: Skipping live HTTP attempt. (Alibaba typically requires JS or triggers challenges on raw HTTP requests)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## 2) Step 2: Escalate to Browser\n",
    "\n",
    "If HTTP fails (e.g., `js_required` or `challenge_detected`), we escalate to the **Browser Engine**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.engines.browser import BrowserEngine, BrowserEngineOptions\n",
    "from scrapping.diagnostics.classifiers import diagnose_rendered_dom\n",
    "\n",
    "if ONLINE:\n",
    "    opts = BrowserEngineOptions(headless=True, save_artifacts=True)\n",
    "    engine = BrowserEngine(options=opts)\n",
    "    \n",
    "    # We add a wait_for selector to ensure products are rendered\n",
    "    res = engine.get_rendered(url, wait_for=\".m-results-item, .item-main\")\n",
    "    diag = diagnose_rendered_dom(res.text or \"\")\n",
    "    \n",
    "    print(f\"Browser Fetch OK: {res.ok} | Diagnosis: {diag.label.value}\")\n",
    "    if not res.ok:\n",
    "        print(f\"Error: {res.short_error()}\")\n",
    "    \n",
    "    engine.close()\n",
    "else:\n",
    "    print(\"OFFLINE: In a real scenario, we'd now see the rendered HTML and artifacts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## 3) Step 3: Define Discovery & Extraction\n",
    "\n",
    "Now we define how to find links and extract product data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.extraction.link_extractors import LinkExtractRequest, extract_links\n",
    "from scrapping.extraction.parsers import select_text_bs4\n",
    "\n",
    "# Sample extraction logic using a mock or real HTML\n",
    "html_source = \"<div class='item-main'><a href='/product/123.html'>Product 1</a><span class='price'>$10</span></div>\"\n",
    "\n",
    "req = LinkExtractRequest(html=html_source, method='regex', pattern=r'/product/\\d+\\.html')\n",
    "links = extract_links(req)\n",
    "print(f\"Extracted Links: {links}\")\n",
    "\n",
    "title = select_text_bs4(html_source, \"a\")\n",
    "price = select_text_bs4(html_source, \".price\")\n",
    "print(f\"Fields: title='{title}', price='{price}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## 4) Step 4: Full Recipe Integration\n",
    "\n",
    "Finally, we wrap everything in the **Alibaba L3 Recipe** for resumability and state management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.recipes.alibaba_l3 import run_single_keyword, AlibabaConfig\n",
    "\n",
    "RUN_DIR = \"results/alibaba_workshop\"\n",
    "config = AlibabaConfig(max_pages=1, checkpoint_every_n=5)\n",
    "\n",
    "print(\"Running full recipe (Offline mode)...\")\n",
    "results = run_single_keyword(\n",
    "    keyword='drone', \n",
    "    output_dir=RUN_DIR, \n",
    "    config=config, \n",
    "    online=False\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(f\"Phase {r.name}: {'‚úÖ' if r.ok else '‚ùå'} in {r.elapsed_ms:.0f}ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "success-def-header",
   "metadata": {},
   "source": [
    "## 5) Defining Success for Hard Targets\n",
    "\n",
    "When a site uses advanced challenges (Turnstile/reCAPTCHA), \"Success\" is defined as:\n",
    "1.  **Correct Detection**: Identification of the challenge.\n",
    "2.  **Artifact Capture**: Saving HTML + Screenshot for legal/technical review.\n",
    "3.  **Graceful Stop**: Avoiding infinite retries or account flagging.\n",
    "\n",
    "**Success Paths**:\n",
    "*   If data is found -> Continue.\n",
    "*   If challenge detected -> Stop + Record.\n",
    "*   If blocked -> Diagnose + Escalate (e.g., to Official API)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
