{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "**Run on Google Colab (Quickstart)**\n",
    "\n",
    "```bash\n",
    "! git clone --branch main --single-branch https://github.com/sbaaihamza/scrapping-lib.git\n",
    "%cd scrapping-lib\n",
    "! pip install -e \".[browser,dev]\"\n",
    "! playwright install\n",
    "# Preferred (installs OS deps automatically on supported distros):\n",
    "! playwright install --with-deps chromium\n",
    "# If needed (manual deps fallback):\n",
    "! apt-get update\n",
    "! apt-get install -y libxcomposite1 libxcursor1 libgtk-3-0 libatk1.0-0 libcairo2 libgdk-pixbuf2.0-0\n",
    "%cd /content/scrapping-lib/notebooks\n",
    "```\n",
    "\n",
    "*Note: Playwright has both sync and async APIs. These notebooks are designed to be async-safe for Jupyter/Colab. If you encounter OS dependency issues, use the `playwright install --with-deps chromium` command.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bedca1",
   "metadata": {},
   "source": [
    "# Engine In-Depth: HTTP Engine Cases\n",
    "\n",
    "This notebook covers the configuration and operation of the `http` engine, ranging from basic page fetches to complex pagination and rate limiting.\n",
    "\n",
    "### Output Directory\n",
    "Results will be written to `results_notebook_http/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e106ef6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821027c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_repo_root(start_path):\n",
    "    p = Path(start_path).resolve()\n",
    "    for parent in [p] + list(p.parents):\n",
    "        if (parent / 'pyproject.toml').exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "sys.path.append(str(REPO_ROOT))\n",
    "os.chdir(str(REPO_ROOT))\n",
    "\n",
    "ONLINE = os.getenv('ONLINE', '0') == '1'\n",
    "RESULTS_DIR = Path('results_notebook_http')\n",
    "print(f'Python version: {sys.version}')\n",
    "print(f'Repo root: {REPO_ROOT}')\n",
    "print(f'Online mode: {ONLINE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae14346",
   "metadata": {},
   "source": [
    "## Case 0: Minimal Public Page\n",
    "HTTP engine is best for static sites that don't require JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a46d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.engines.http import HttpEngine\n",
    "from scrapping.extraction.link_extractors import LinkExtractRequest, extract_links\n",
    "\n",
    "url = 'https://quotes.toscrape.com/'\n",
    "if ONLINE:\n",
    "    engine = HttpEngine()\n",
    "    res = engine.get(url)\n",
    "    if res.ok:\n",
    "        html = res.text\n",
    "    else:\n",
    "        print(f\"‚ùå Fetch failed: {res.short_error()}\")\n",
    "        html = \"\"\n",
    "else:\n",
    "    print('ONLINE=0: using fixtures')\n",
    "    fixture_path = 'tests/fixtures/html/listing_quotes.html'\n",
    "    if Path(fixture_path).exists():\n",
    "        html = Path(fixture_path).read_text()\n",
    "    else:\n",
    "        html = \"\"\n",
    "\n",
    "if html:\n",
    "    req = LinkExtractRequest(html=html, method='css', selector='.quote span a')\n",
    "    links = extract_links(req)\n",
    "    print(f'Found {len(links)} links. Samples: {links[:3]}')\n",
    "else:\n",
    "    print(\"No HTML to parse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "link-extraction-patterns",
   "metadata": {},
   "source": [
    "## Case 1: Advanced Link Extraction\n",
    "Moving beyond basic CSS selectors to robust patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-links",
   "metadata": {},
   "outputs": [],
   "source": "import re\n\nfrom scrapping.extraction.link_extractors import LinkExtractRequest, extract_links, normalize_url\n\n# 1. Regex extraction (Absolute URLs)\nhtml_abs = '<a href=\"https://example.com/p/1\">P1</a> <a href=\"https://example.com/p/2\">P2</a>'\nreq_abs = LinkExtractRequest(html=html_abs, method='regex', pattern=r'https://example\\.com/p/\\d+')\nprint(f\"Regex (Absolute): {extract_links(req_abs)}\")\n\n# 2. Regex extraction (Relative URLs + Base URL join)\nhtml_rel = '<a href=\"/p/1\">P1</a> <a href=\"/p/2\">P2</a>'\nreq_rel = LinkExtractRequest(html=html_rel, base_url=\"https://example.com\", method='regex', pattern=r'/p/\\d+')\nprint(f\"Regex (Relative): {extract_links(req_rel)}\")\n\n# 3. Include/Exclude filters\nall_links = ['/p/1', '/p/2', '/about', '/contact']\ninclude = r'/p/\\d+'\n\nfiltered = [link for link in all_links if re.search(include, link)]\nprint(f\"Filtered (Include /p/\\d+): {filtered}\")\n\n# 4. Canonicalization\nurls = [\n    \"https://example.com/p/1#fragment\",\n    \"https://example.com/p/1/\",\n    \"https://example.com/p/1?utm_source=fb\"\n]\nnormalized = [normalize_url(u, drop_fragments=True, drop_tracking_params=True) for u in urls]\nprint(f\"Normalized: {normalized}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Verify the regex patterns above\n",
    "test_abs = \"https://example.com/p/123\"\n",
    "pattern_abs = r'https://example\\.com/p/\\d+'\n",
    "matches_abs = re.findall(pattern_abs, test_abs)\n",
    "print(f\"Absolute regex check: {matches_abs}\")\n",
    "assert len(matches_abs) == 1\n",
    "\n",
    "test_rel = \"/p/456\"\n",
    "pattern_rel = r'/p/\\d+'\n",
    "matches_rel = re.findall(pattern_rel, test_rel)\n",
    "print(f\"Relative regex check: {matches_rel}\")\n",
    "assert len(matches_rel) == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pagination-patterns",
   "metadata": {},
   "source": [
    "## Case 2: Pagination Patterns\n",
    "Learn how to handle template-based and discovery-based pagination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pagination-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.pipeline.stages import discover_listing_urls\n",
    "\n",
    "# 1. Template: Page path\n",
    "cfg_page = {\n",
    "    'entrypoints': [{ 'url': 'https://quotes.toscrape.com/page/{page}/', 'paging': {'mode': 'page', 'start': 1, 'pages': 3} }]\n",
    "}\n",
    "print(f\"Page Template: {discover_listing_urls(cfg_page)}\")\n",
    "\n",
    "# 2. Template: Offset\n",
    "cfg_offset = {\n",
    "    'entrypoints': [{ 'url': 'https://httpbin.org/get?start={offset}', 'paging': {'mode': 'offset', 'start': 0, 'step': 10, 'pages': 3} }]\n",
    "}\n",
    "print(f\"Offset Template: {discover_listing_urls(cfg_offset)}\")\n",
    "\n",
    "# 3. Discovery: Next-link\n",
    "def extract_next_link(html, base_url):\n",
    "    req = LinkExtractRequest(html=html, base_url=base_url, method='css', selector='li.next a')\n",
    "    links = extract_links(req)\n",
    "    return links[0] if links else None\n",
    "\n",
    "if ONLINE:\n",
    "    res = HttpEngine().get('https://quotes.toscrape.com/')\n",
    "    next_page = extract_next_link(res.text, res.final_url)\n",
    "    print(f\"Discovered Next Page: {next_page}\")\n",
    "else:\n",
    "    print(\"Next Page extraction: (needs online or fixture with next link)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602f538",
   "metadata": {},
   "source": [
    "## Case 3: Rate Limiting & Retries\n",
    "Handling high-load targets safely and respectfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.engines.http import HttpEngine, HttpEngineOptions\n",
    "\n",
    "# 1. Retry behavior on status codes\n",
    "if ONLINE:\n",
    "    url_429 = 'https://httpbin.org/status/429'\n",
    "    # Exponential backoff with jitter is the default\n",
    "    opts = HttpEngineOptions(max_retries=2, base_delay_s=1.0, backoff_mode='exp')\n",
    "    engine = HttpEngine(options=opts)\n",
    "    print('Fetching 429 endpoint (will retry with exp backoff)...')\n",
    "    res = engine.get(url_429)\n",
    "    print(f'Final status: {res.status_code} after {len(res.engine_trace)} retries')\n",
    "else:\n",
    "    print('Offline: Skipping 429 demo')\n",
    "\n",
    "# 2. Rate Limiting (RPS control)\n",
    "opts_rl = HttpEngineOptions(rps=0.5) # 1 request every 2 seconds\n",
    "engine_rl = HttpEngine(options=opts_rl)\n",
    "print(\"Engine configured with 0.5 RPS (compliance-first)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b9289",
   "metadata": {},
   "source": [
    "## Case 4: Block Detection\n",
    "Identify when a site is blocking traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8da56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.runtime.blocks import classify_blocks\n",
    "\n",
    "html_blocked = \"<html><title>Access Denied</title><body>Please solve CAPTCHA to continue.</body></html>\"\n",
    "signals = classify_blocks(html_blocked)\n",
    "print(f\"Detection signals: {signals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46cdd0",
   "metadata": {},
   "source": [
    "## Final Trial: http_quotes.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb68dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.orchestrator import Orchestrator, OrchestratorOptions\n",
    "\n",
    "with open('examples/configs/real/http_quotes.json') as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "if ONLINE:\n",
    "    orch = Orchestrator(options=OrchestratorOptions(results_dir=RESULTS_DIR, parallelism=1))\n",
    "    # Limit for trial\n",
    "    cfg['sources'][0]['entrypoints'][0]['paging']['pages'] = 1\n",
    "    out = orch.run(cfg)\n",
    "    print(json.dumps(out, indent=2))\n",
    "else:\n",
    "    print('ONLINE=0: skipping online trial')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}