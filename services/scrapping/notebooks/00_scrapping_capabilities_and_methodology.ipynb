{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "**Run on Google Colab (Quickstart)**\n",
    "\n",
    "```bash\n",
    "! git clone --branch main --single-branch https://github.com/sbaaihamza/scrapping-lib.git\n",
    "%cd scrapping-lib\n",
    "! pip install -e \".[browser,dev]\"\n",
    "! playwright install\n",
    "# Preferred (installs OS deps automatically on supported distros):\n",
    "! playwright install --with-deps chromium\n",
    "# If needed (manual deps fallback):\n",
    "! apt-get update\n",
    "! apt-get install -y libxcomposite1 libxcursor1 libgtk-3-0 libatk1.0-0 libcairo2 libgdk-pixbuf2.0-0\n",
    "%cd /content/scrapping-lib/notebooks\n",
    "```\n",
    "\n",
    "*Note: Playwright has both sync and async APIs. These notebooks are designed to be async-safe for Jupyter/Colab. If you encounter OS dependency issues, use the `playwright install --with-deps chromium` command.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Scrapping Library: Capabilities & Methodology\n",
    "\n",
    "Welcome to the `scrapping` library. This notebook is a **hands-on workshop** that teaches you the full extent of the library's capabilities and the recommended **Methodology Ladder** for building robust, compliant scrapers.\n",
    "\n",
    "## 0) What you will learn (outcomes)\n",
    "\n",
    "*   **Capability Map**: Everything the package offers (engines, pipeline, recipes, QA, observability).\n",
    "*   **Methodology Ladder**: The structured \"Try-First to Try-Last\" escalation path.\n",
    "*   **Diagnostics Lab**: Hands-on analysis of \"Hard Targets\" using the `diagnostics` module.\n",
    "*   **Compliance-First Guardrails**: How to detect challenges and stop gracefully.\n",
    "*   **Maintenance Loop**: Turning online exploration into stable offline fixtures and tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capability-map",
   "metadata": {},
   "source": [
    "## 1) Capability Map\n",
    "\n",
    "The library is designed to balance speed, complexity, and reliability.\n",
    "\n",
    "| Feature | Cost/Complexity | failure modes | Artifacts to Inspect | Notebook |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **HTTP Engine** | Low | Missing JS content, 403/429. | raw HTML. | 03 |\n",
    "| **Browser Engine** | High | Slow, resource heavy, binaries missing. | Screenshots, HTML, Traces. | 04 |\n",
    "| **Hybrid Engine** | Medium | Complexity in routing. | Combined traces. | 05 |\n",
    "| **Paging** | Low-Medium | Missing items on deep pages. | URL lists. | 07 |\n",
    "| **Actions DSL** | Medium | Broken selectors, timeouts. | Screenshot on error. | 04 |\n",
    "| **QA Filters** | Low | Too strict (drops good data). | `rejected.jsonl`. | 01 |\n",
    "| **Recipes** | High | State corruption. | `state.json`. | 06 |\n",
    "| **Observability** | Low | Misconfigured loggers. | `run_report.json`. | 01 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methodology-ladder",
   "metadata": {},
   "source": [
    "## 2) Methodology Ladder (Try-First â†’ Try-Last)\n",
    "\n",
    "Escalate your strategy only when a specific signal triggers the need.\n",
    "\n",
    "### Escalation Flow\n",
    "1.  **HTTP Minimal**: Start here. If `Ctrl+U` in browser shows the data, use `HttpEngine`.\n",
    "2.  **Robust Extraction**: Use CSS/Regex + `normalize_url` to handle fragments and tracking params.\n",
    "3.  **Paging**: Add pagination templates (e.g., `{page}`) if you need more than one page.\n",
    "4.  **QA + Dedupe**: Add Pydantic schemas and URL/Content deduping for data quality.\n",
    "5.  **Rate Limiting**: Add `rps` and `min_delay_s` to avoid 429 errors.\n",
    "6.  **Browser Engine**: Switch here if content only appears after JS execution.\n",
    "7.  **Hybrid Engine**: Use HTTP for fast listings and Browser for detail rendering.\n",
    "8.  **Recipes**: Wrap in a Recipe if you need linear phases and resumability.\n",
    "9.  **Fixtures**: Capture HTML fixtures once the scraper works to prevent future drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 3) Setup & Diagnostics Lab\n",
    "\n",
    "In this section, we use the `diagnostics` module to practice professional diagnostics against \"Hard Targets\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "### Helper: Resolving Run Reports\n",
    "\n",
    "The Orchestrator might return the full report object or just paths to it. We use this helper to ensure we can always access source results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resolve-report-helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_run_report(out):\n",
    "    \"\"\"Robustly load run_report.json from orchestrator output.\"\"\"\n",
    "    if isinstance(out, dict) and \"sources\" in out and isinstance(out[\"sources\"], list):\n",
    "        # If it's already a full report dict with list of sources\n",
    "        return out\n",
    "    \n",
    "    if isinstance(out, dict) and \"run_report_path\" in out:\n",
    "        with open(out[\"run_report_path\"]) as f:\n",
    "            return json.load(f)\n",
    "            \n",
    "    if isinstance(out, dict) and \"run_dir\" in out:\n",
    "        report_path = Path(out[\"run_dir\"]) / \"run_report.json\"\n",
    "        if report_path.exists():\n",
    "            with open(report_path) as f:\n",
    "                return json.load(f)\n",
    "    \n",
    "    print(f\"Warning: Could not resolve report automatically. Output shape: {list(out.keys()) if isinstance(out, dict) else type(out)}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scrapping.diagnostics.classifiers import diagnose_http_response, recommend_next_step\n",
    "from scrapping.orchestrator import Orchestrator, OrchestratorOptions\n",
    "\n",
    "REPO_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(REPO_ROOT))\n",
    "os.chdir(str(REPO_ROOT))\n",
    "\n",
    "ONLINE = os.getenv('ONLINE', '0') == '1'\n",
    "HARD_TARGETS = os.getenv('HARD_TARGETS', '0') == '1'\n",
    "\n",
    "print(f'Online mode: {ONLINE} | Hard Targets: {HARD_TARGETS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostics-demo",
   "metadata": {},
   "source": [
    "### 3.1 Standardized Diagnostics\n",
    "\n",
    "The `diagnostics` module helps you decide the next step based on the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnose-signals",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    (429, {}, \"Too many requests\"),\n",
    "    (200, {}, \"Enable JavaScript to continue\"),\n",
    "    (200, {}, \"Cloudflare Turnstile verification\"),\n",
    "    (403, {}, \"Access Denied\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for status, headers, text in scenarios:\n",
    "    diag = diagnose_http_response(status, headers, text)\n",
    "    results.append({\n",
    "        \"Label\": diag.label.value, \n",
    "        \"Reason\": diag.reason,\n",
    "        \"Recommendation\": recommend_next_step(diag)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hard-targets-bot",
   "metadata": {},
   "source": [
    "### 3.2 Bot-Detection Diagnostics\n",
    "\n",
    "We test our engine against bot-detectors to analyze which signals (like `webdriver` or `headless`) are flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bot-diagnostics-lab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'examples/configs/real/hard_targets/browser_bot_diagnostics.json'\n",
    "with open(config_path) as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "if ONLINE and HARD_TARGETS:\n",
    "    orch = Orchestrator(options=OrchestratorOptions(results_dir='results/hard_targets'))\n",
    "    out = orch.run(cfg)\n",
    "    \n",
    "    report = resolve_run_report(out)\n",
    "    summary = []\n",
    "    \n",
    "    # Handle both dict and list sources depending on report version\n",
    "    sources = report.get('sources', [])\n",
    "    if isinstance(sources, list):\n",
    "        for s_res in sources:\n",
    "            summary.append({\n",
    "                \"Target\": s_res.get('source_id'), \n",
    "                \"Diagnosis\": s_res.get('diagnosis', {}).get('label'),\n",
    "                \"Recommendation\": s_res.get('diagnosis', {}).get('recommendation'),\n",
    "                \"Artifacts\": s_res.get('artifacts', {}).get('raw_detail_parts', [])[:1]\n",
    "            })\n",
    "    \n",
    "    print(\"--- Hard Targets Diagnostics Lab Results ---\")\n",
    "    print(pd.DataFrame(summary))\n",
    "else:\n",
    "    print(\"OFFLINE: Diagnostics lab skipped. Enable ONLINE=1 and HARD_TARGETS=1 for live analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hard-targets-challenge",
   "metadata": {},
   "source": [
    "### 3.3 Challenge Detection (Compliance-First)\n",
    "\n",
    "**Policy**: Success == Detection + Graceful Stop. Do not attempt bypass.\n",
    "\n",
    "**Important Note**: Cloudflare explicitly states that automated testing suites (like Playwright/Selenium) are often detected as bots by Turnstile. This is the intended behavior of such security measures. Our goal is professional diagnosis and safe behavior: detecting the wall, recording the event, and stopping execution to avoid further escalation or account flagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-lab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.diagnostics.classifiers import diagnose_rendered_dom\n",
    "\n",
    "html_fixture = \"<html><body><div id='turnstile-widget'></div></body></html>\"\n",
    "diag = diagnose_rendered_dom(html_fixture)\n",
    "\n",
    "print(f\"Target: Turnstile Demo | Label: {diag.label.value} | Action: {recommend_next_step(diag)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maintenance-loop",
   "metadata": {},
   "source": [
    "## 4) Maintenance Loop: Exploration to Regression\n",
    "\n",
    "Professional scraping requires locking in successes as offline tests.\n",
    "\n",
    "1.  **Diagnose**: Ensure the page loads correctly and content is present.\n",
    "2.  **Capture**: Use the CLI to save the HTML as a fixture.\n",
    "    ```bash\n",
    "    scrap capture-fixture --url https://example.com --out tests/fixtures/html/example.html\n",
    "    ```\n",
    "3.  **Scaffold**: Generate a regression test asserting extraction works on that fixture.\n",
    "    ```bash\n",
    "    scrap scaffold-test --fixture tests/fixtures/html/example.html --extract css --pattern 'h1' --expect-count 1\n",
    "    ```\n",
    "4.  **Commit**: Push the fixture and test to your repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "runnable-examples",
   "metadata": {},
   "source": [
    "## 5) Minimal Runnable Examples (Offline)\n",
    "\n",
    "### 5.1 HTTP Minimal (books.toscrape.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-http",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.engines.http import HttpEngine\n",
    "\n",
    "engine = HttpEngine()\n",
    "url = \"https://books.toscrape.com/\"\n",
    "if ONLINE:\n",
    "    res = engine.get(url)\n",
    "    print(f\"Fetch OK: {res.ok} | status: {res.status_code}\")\n",
    "else:\n",
    "    print(\"OFFLINE: Skipping live fetch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-checklist",
   "metadata": {},
   "source": [
    "### Final Checklist for New Scrapers\n",
    "- [ ] Did I try **HTTP first**?\n",
    "- [ ] Did the **Diagnosis** confirm it's safe to proceed?\n",
    "- [ ] Have I added **rate limiting** (politeness)?\n",
    "- [ ] Have I saved an **offline fixture** for regression testing?\n",
    "\n",
    "### Final Checklist for Advanced / Hard Targets\n",
    "- [ ] OS dependencies verified (`playwright install --with-deps chromium`).\n",
    "- [ ] Diagnostics label exists for every failure mode.\n",
    "- [ ] Stop condition implemented for challenges (no infinite retries).\n",
    "- [ ] Artifacts captured on every failure (HTML + Screenshot).\n",
    "- [ ] Fixture captured + regression test scaffolded.\n",
    "- [ ] Idempotency verified (no duplicates in output).\n",
    "- [ ] Observability review completed (`run_report.json` checked)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
