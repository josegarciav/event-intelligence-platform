{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run on Google Colab (Quickstart)**\n",
    "\n",
    "```bash\n",
    "! git clone --branch main --single-branch https://github.com/sbaaihamza/scrapping-lib.git\n",
    "%cd scrapping-lib\n",
    "! pip install -e \".[browser,dev]\"\n",
    "! playwright install\n",
    "# Preferred (installs OS deps automatically on supported distros):\n",
    "! playwright install --with-deps chromium\n",
    "# If needed (manual deps fallback):\n",
    "! apt-get update\n",
    "! apt-get install -y libxcomposite1 libxcursor1 libgtk-3-0 libatk1.0-0 libcairo2 libgdk-pixbuf2.0-0\n",
    "%cd /content/scrapping-lib/notebooks\n",
    "```\n",
    "\n",
    "*Note: Playwright has both sync and async APIs. These notebooks are designed to be async-safe for Jupyter/Colab. If you encounter OS dependency issues, use the `playwright install --with-deps chromium` command.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Jobs Aggregator: Multi-Source Methodology\n",
    "\n",
    "This notebook is an operational guide for scaling your scraping operations across multiple job sites using the `Jobs Aggregator` recipe.\n",
    "\n",
    "## 0) What this notebook teaches\n",
    "\n",
    "*   **Configuration Strategy**: How to move from flat configs to structured, source-specific objects.\n",
    "*   **Patterns**: Deep dive into **Paging** and **Link Extraction** strategies.\n",
    "*   **Lab**: A step-by-step workshop on **onboarding a new job site** in under 30 minutes.\n",
    "*   **Quality**: Managing **JobPostItem** validation and rejection flows.\n",
    "*   **Observability**: Monitoring multi-source runs via `jobs_tracking.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(REPO_ROOT))\n",
    "os.chdir(str(REPO_ROOT))\n",
    "\n",
    "ONLINE = os.getenv('ONLINE', '0') == '1'\n",
    "RUN_DIR = Path('results/jobs_nb_guided')\n",
    "\n",
    "if RUN_DIR.exists(): shutil.rmtree(RUN_DIR)\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Online mode: {ONLINE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-anatomy",
   "metadata": {},
   "source": [
    "## 1) Config Anatomy: The `JobSourceConfig` Object\n",
    "\n",
    "Each site is defined by a `JobSourceConfig`. Here are the primary blocks:\n",
    "\n",
    "| Block | Description | Key Fields |\n",
    "| :--- | :--- | :--- |\n",
    "| **Engine** | How to fetch content | `type` (http/browser), `timeout_s`, `verify_ssl` |\n",
    "| **Entrypoints** | Where to start | `url` template, `paging` (mode, pages, step) |\n",
    "| **Discovery** | How to find jobs | `link_extract` (method, pattern/selector) |\n",
    "| **Parsing** | How to extract fields | `item_extract` (selectors for title, company, etc.) |\n",
    "| **Policies** | Quality controls | `checkpoint_every_n`, `min_description_len` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paging-patterns",
   "metadata": {},
   "source": [
    "## 2) Paging Patterns\n",
    "\n",
    "The recipe supports several pagination strategies out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paging-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.recipes.jobs_aggregator import DiscoverListingPagesPhase, JobRecipeContext, JobSourceConfig, StateManager\n",
    "import logging\n",
    "\n",
    "# 1. Page template (?page=1)\n",
    "cfg_page = JobSourceConfig(source_id=\"page_site\", entrypoints=[{\"url\": \"http://site.com/jobs?p={page}\", \"paging\": {\"mode\": \"page\", \"pages\": 3}}])\n",
    "\n",
    "# 2. Offset template (0, 10, 20...)\n",
    "cfg_offset = JobSourceConfig(source_id=\"offset_site\", entrypoints=[{\"url\": \"http://site.com/api?start={offset}\", \"paging\": {\"mode\": \"offset\", \"pages\": 3, \"step\": 10}}])\n",
    "\n",
    "def preview_paging(cfg):\n",
    "    state = StateManager(output_dir=\"tmp/paging_test\")\n",
    "    ctx = JobRecipeContext(engine=None, state=state, config=cfg, online=False, log=logging.getLogger(\"test\"))\n",
    "    DiscoverListingPagesPhase().run(ctx)\n",
    "    return state.metadata['listing_urls']\n",
    "\n",
    "print(f\"Page Mode URLs: {preview_paging(cfg_page)}\")\n",
    "print(f\"Offset Mode URLs: {preview_paging(cfg_offset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "link-extraction-patterns",
   "metadata": {},
   "source": [
    "## 3) Link Extraction Patterns\n",
    "\n",
    "Configuring how to find job links correctly is the most important step for a successful run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "link-extract-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.extraction.link_extractors import LinkExtractRequest, extract_links\n",
    "\n",
    "html = \"\"\"\n",
    "<div class='job'><a href='/j/101'>Software Engineer</a></div>\n",
    "<div class='job'><a href='https://other.com/jobs/202'>Data Scientist</a></div>\n",
    "<div class='ad'><a href='/promo'>Buy our coffee</a></div>\n",
    "\"\"\"\n",
    "\n",
    "# 1. CSS Extraction (Targeted)\n",
    "req_css = LinkExtractRequest(html=html, base_url=\"https://mysite.com\", method=\"css\", selector=\".job a\")\n",
    "print(f\"CSS Links: {extract_links(req_css)}\")\n",
    "\n",
    "# 2. Regex Extraction (Specific Pattern)\n",
    "req_rx = LinkExtractRequest(html=html, base_url=\"https://mysite.com\", method=\"regex\", pattern=r\"/j/\\d+\")\n",
    "print(f\"Regex Links: {extract_links(req_rx)}\")\n",
    "\n",
    "# 3. Normalization (Clean trailing slash and fragments)\n",
    "req_norm = LinkExtractRequest(html=\"<a href='/j/1/#frag'>Job 1</a>\", base_url=\"https://mysite.com\", method=\"css\", selector=\"a\", normalize=True)\n",
    "print(f\"Normalized Link: {extract_links(req_norm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Verify the jobs link regex\n",
    "test_job = \"/j/999\"\n",
    "pattern_job = r\"/j/\\d+\"\n",
    "matches_job = re.findall(pattern_job, test_job)\n",
    "print(f\"Jobs regex check: {matches_job}\")\n",
    "assert len(matches_job) == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-quality",
   "metadata": {},
   "source": [
    "## 4) Job Schema & Quality Filters\n",
    "\n",
    "Unified data is ensured by the `JobPostItem` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.schemas.job_items import JobPostItem\n",
    "\n",
    "job = JobPostItem(\n",
    "    source_id=\"linkedin\",\n",
    "    url=\"http://linkedin.com/j1\",\n",
    "    title=\"Python Developer\",\n",
    "    company=\"Tech Corp\",\n",
    "    location=\"Remote\",\n",
    "    description=\"Must know Python and scrapers...\" * 20\n",
    ")\n",
    "print(f\"Valid Job: {job.title} at {job.company}\")\n",
    "\n",
    "# Rejection Reason Example\n",
    "print(f\"\\nIf 'description' is < 50 chars, it will be saved to 'jobs_rejected.jsonl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-lab",
   "metadata": {},
   "source": [
    "## 5) \"Add a New Site\" Guided Lab\n",
    "\n",
    "Follow these steps to onboard a new target:\n",
    "\n",
    "### Step A: Start from Template\n",
    "```python\n",
    "new_site = {\n",
    "    \"source_id\": \"my_new_job_site\",\n",
    "    \"engine\": {\"type\": \"http\"}, # Step B\n",
    "    \"entrypoints\": [{\"url\": \"...\", \"paging\": {\"mode\": \"page\", \"pages\": 1}}],\n",
    "    \"discovery\": {\"link_extract\": {\"method\": \"regex\", \"pattern\": \"...\"}}, # Step C\n",
    "    \"parsing\": {\"item_extract\": {\"fields\": {\"title\": {\"selector\": \"h1\"}}}} # Step D\n",
    "}\n",
    "```\n",
    "\n",
    "### Step B: Engine Triage\n",
    "*   Use **HTTP** if the HTML contains the job data in the raw source (`Ctrl+U`).\n",
    "*   Use **Browser** if content only appears after 2-3 seconds or requires scrolling.\n",
    "\n",
    "### Step E: Run Offline on Fixture\n",
    "Place a sample listing in `tests/fixtures/html/jobs/my_new_job_site/listing.html` and run the recipe with `online=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lab-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapping.recipes.jobs_aggregator import run_jobs_recipe\n",
    "\n",
    "LAB_DIR = RUN_DIR / \"lab_trial\"\n",
    "cfg = JobSourceConfig(\n",
    "    source_id=\"quotes_jobs_mock\", \n",
    "    entrypoints=[{\"url\": \"http://quotes.toscrape.com\", \"paging\": {\"mode\": \"page\", \"pages\": 1}}],\n",
    "    discovery={\"link_extract\": {\"method\": \"css\", \"selector\": \".quote a\"}},\n",
    "    parsing={\"item_extract\": {\"fields\": {\"title\": {\"selector\": \".text\"}, \"company\": {\"selector\": \".author\"}}}}\n",
    ")\n",
    "\n",
    "run_jobs_recipe([cfg], output_root=str(LAB_DIR), online=False)\n",
    "print(f\"Lab run complete. Check {LAB_DIR}/quotes_jobs_mock/jobs.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "observability",
   "metadata": {},
   "source": [
    "## 6) Observability & Artifacts Inspection\n",
    "\n",
    "Every multi-source run generates a `jobs_tracking.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-tracking",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_file = LAB_DIR / \"jobs_tracking.json\"\n",
    "if tracking_file.exists():\n",
    "    tracking = json.loads(tracking_file.read_text())\n",
    "    for sid, data in tracking.items():\n",
    "        if isinstance(data, dict):\n",
    "            print(f\"Source: {sid} | Status: {data.get('status')} | Results: {len(data.get('results', []))} phases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-checklist",
   "metadata": {},
   "source": [
    "### Summary Checklist for New Sources\n",
    "- [ ] Config defined and validated.\n",
    "- [ ] Engine selected (HTTP vs Browser).\n",
    "- [ ] Link extraction regex/selector verified.\n",
    "- [ ] Detail page selectors verified.\n",
    "- [ ] Offline run successful on fixtures.\n",
    "- [ ] Regression test added to test suite."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}