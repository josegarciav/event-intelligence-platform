{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Multi-Source Event Ingestion Pipeline\n",
    "\n",
    "This notebook tests **both Ra.co and Ticketmaster** pipelines running simultaneously.\n",
    "All sources are created through `PipelineFactory` using YAML configuration.\n",
    "\n",
    "**Pipeline flow:**\n",
    "1. Factory creates both `ra_co` (GraphQL) and `ticketmaster` (REST) pipelines from config\n",
    "2. Each pipeline fetches raw data via its adapter\n",
    "3. FieldMapper extracts + transforms fields per source config\n",
    "4. TaxonomyMapper assigns Human Experience Taxonomy dimensions\n",
    "5. Events from both sources are combined, deduplicated, and compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:55:30.480981Z",
     "iopub.status.busy": "2026-02-18T10:55:30.480751Z",
     "iopub.status.idle": "2026-02-18T10:55:30.493682Z",
     "shell.execute_reply": "2026-02-18T10:55:30.493013Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup path — point to services/api so src.* imports work\n",
    "API_ROOT = os.path.abspath(os.path.join(\"..\", \"services\", \"api\"))\n",
    "if API_ROOT not in sys.path:\n",
    "    sys.path.insert(0, API_ROOT)\n",
    "\n",
    "# Load .env from services/api so API keys are available\n",
    "from dotenv import load_dotenv\n",
    "env_path = os.path.join(API_ROOT, \".env\")\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Verify key env vars are loaded (without printing the values)\n",
    "tm_key = os.environ.get(\"TICKETMASTER_API_KEY\", \"\")\n",
    "print(f\"API root: {API_ROOT}\")\n",
    "print(f\"TICKETMASTER_API_KEY loaded: {'yes (' + str(len(tm_key)) + ' chars)' if tm_key else 'NO - check .env'}\")\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "factory-section",
   "metadata": {},
   "source": [
    "## Step 1: PipelineFactory — List All Configured Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "factory-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:55:30.495267Z",
     "iopub.status.busy": "2026-02-18T10:55:30.495158Z",
     "iopub.status.idle": "2026-02-18T10:55:30.683342Z",
     "shell.execute_reply": "2026-02-18T10:55:30.682978Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.ingestion.factory import PipelineFactory\n",
    "\n",
    "factory = PipelineFactory()\n",
    "\n",
    "print(\"Configured Sources:\")\n",
    "print(\"=\" * 50)\n",
    "for name, info in factory.list_sources().items():\n",
    "    status = \"ENABLED\" if info[\"enabled\"] else \"disabled\"\n",
    "    print(f\"  {name:20} type={info['type']:10} [{status}]\")\n",
    "\n",
    "enabled = factory.list_enabled_sources()\n",
    "print(f\"\\nEnabled sources: {enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-pipelines-section",
   "metadata": {},
   "source": [
    "## Step 2: Create Both Pipelines\n",
    "\n",
    "- **Ra.co**: GraphQL API, uses `defaults.areas` (dict city → area_id), 1-indexed pagination\n",
    "- **Ticketmaster**: REST API, uses `defaults.cities` (list), 0-indexed pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-pipelines",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:55:30.684337Z",
     "iopub.status.busy": "2026-02-18T10:55:30.684273Z",
     "iopub.status.idle": "2026-02-18T10:55:30.696453Z",
     "shell.execute_reply": "2026-02-18T10:55:30.696137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create all enabled API pipelines at once\n",
    "pipelines = factory.create_all_enabled_pipelines()\n",
    "\n",
    "ra_co = pipelines[\"ra_co\"]\n",
    "ticketmaster = pipelines[\"ticketmaster\"]\n",
    "\n",
    "print(\"Ra.co pipeline:\")\n",
    "print(f\"  Protocol:    {ra_co.source_config.protocol}\")\n",
    "print(f\"  Endpoint:    {ra_co.source_config.endpoint}\")\n",
    "print(f\"  Areas:       {ra_co.source_config.defaults.get('areas', {})}\")\n",
    "print(f\"  Days ahead:  {ra_co.source_config.defaults.get('days_ahead')}\")\n",
    "print(f\"  Start page:  {ra_co.source_config.pagination_start_page}\")\n",
    "\n",
    "print(\"\\nTicketmaster pipeline:\")\n",
    "print(f\"  Protocol:    {ticketmaster.source_config.protocol}\")\n",
    "print(f\"  Endpoint:    {ticketmaster.source_config.endpoint}\")\n",
    "print(f\"  Cities:      {ticketmaster.source_config.defaults.get('cities', [])}\")\n",
    "print(f\"  Days ahead:  {ticketmaster.source_config.defaults.get('days_ahead')}\")\n",
    "print(f\"  Start page:  {ticketmaster.source_config.pagination_start_page}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute-raco-section",
   "metadata": {},
   "source": [
    "## Step 3: Execute Ra.co Pipeline\n",
    "\n",
    "Limited to Barcelona, 1 page of 10 events for a quick smoke test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-raco",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:55:30.697405Z",
     "iopub.status.busy": "2026-02-18T10:55:30.697350Z",
     "iopub.status.idle": "2026-02-18T10:57:17.574656Z",
     "shell.execute_reply": "2026-02-18T10:57:17.574015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Limit scope for notebook speed: Barcelona only, 1 page\n",
    "# ra_co.source_config.defaults[\"areas\"] = {\"Barcelona\": 20}\n",
    "\n",
    "raco_result = await ra_co.execute(max_pages=1, page_size=10)\n",
    "\n",
    "print(\"Ra.co Pipeline Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Status:          {raco_result.status.value}\")\n",
    "print(f\"Raw events:      {raco_result.total_events_processed}\")\n",
    "print(f\"Successful:      {raco_result.successful_events}\")\n",
    "print(f\"Failed:          {raco_result.failed_events}\")\n",
    "print(f\"Duration:        {raco_result.duration_seconds:.2f}s\")\n",
    "print(f\"Success rate:    {raco_result.success_rate:.1f}%\")\n",
    "print(f\"Cities:          {raco_result.metadata.get('cities', [])}\")\n",
    "\n",
    "if raco_result.errors:\n",
    "    print(f\"\\nErrors: {raco_result.errors[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute-tm-section",
   "metadata": {},
   "source": [
    "## Step 4: Execute Ticketmaster Pipeline\n",
    "\n",
    "Limited to Barcelona, 1 page of 10 events. Note: Ticketmaster uses 0-indexed pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-ticketmaster",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:17.576610Z",
     "iopub.status.busy": "2026-02-18T10:57:17.576463Z",
     "iopub.status.idle": "2026-02-18T10:57:18.424161Z",
     "shell.execute_reply": "2026-02-18T10:57:18.423279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Limit scope: Barcelona only, 1 page\n",
    "# ticketmaster.source_config.defaults[\"cities\"] = [\"Barcelona\"]\n",
    "\n",
    "tm_result = await ticketmaster.execute(max_pages=1, page_size=10)\n",
    "\n",
    "print(\"Ticketmaster Pipeline Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Status:          {tm_result.status.value}\")\n",
    "print(f\"Raw events:      {tm_result.total_events_processed}\")\n",
    "print(f\"Successful:      {tm_result.successful_events}\")\n",
    "print(f\"Failed:          {tm_result.failed_events}\")\n",
    "print(f\"Duration:        {tm_result.duration_seconds:.2f}s\")\n",
    "print(f\"Success rate:    {tm_result.success_rate:.1f}%\")\n",
    "print(f\"Cities:          {tm_result.metadata.get('cities', [])}\")\n",
    "\n",
    "if tm_result.errors:\n",
    "    print(f\"\\nErrors: {tm_result.errors[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-tm-section",
   "metadata": {},
   "source": [
    "## Step 5: Inspect Ticketmaster Events\n",
    "\n",
    "Verify field mapping is correct: title, venue, artists, price, classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-tm-events",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:18.426507Z",
     "iopub.status.busy": "2026-02-18T10:57:18.426343Z",
     "iopub.status.idle": "2026-02-18T10:57:18.433527Z",
     "shell.execute_reply": "2026-02-18T10:57:18.432435Z"
    }
   },
   "outputs": [],
   "source": [
    "tm_events = tm_result.events\n",
    "\n",
    "if tm_events:\n",
    "    print(f\"Ticketmaster events ({len(tm_events)} total):\")\n",
    "    print(\"=\" * 70)\n",
    "    for i, event in enumerate(tm_events[:10]):\n",
    "        print(f\"\\n[{i+1}] {event.title}\")\n",
    "        print(f\"    City:    {event.location.city} | Venue: {event.location.venue_name}\")\n",
    "        print(f\"    Date:    {event.start_datetime}\")\n",
    "        print(f\"    Type:    {event.event_type}\")\n",
    "        price = event.price\n",
    "        if price and price.minimum_price is not None:\n",
    "            price_str = f\"{price.minimum_price}–{price.maximum_price} {price.currency}\"\n",
    "        elif price and price.price_raw_text:\n",
    "            price_str = price.price_raw_text\n",
    "        else:\n",
    "            price_str = \"N/A\"\n",
    "        print(f\"    Price:   {price_str}\")\n",
    "        print(f\"    Artists: {[a.name for a in event.artists]}\")\n",
    "        print(f\"    Source:  {event.source.source_url}\")\n",
    "        if event.location.coordinates:\n",
    "            print(f\"    Coords:  ({event.location.coordinates.latitude}, {event.location.coordinates.longitude})\")\n",
    "        print(f\"    Quality: {event.data_quality_score:.2f}\")\n",
    "        print(f\"    Custom:  {event.custom_fields}\")\n",
    "else:\n",
    "    print(\"No Ticketmaster events. Check pipeline logs above for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-raco-events",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:18.435399Z",
     "iopub.status.busy": "2026-02-18T10:57:18.435240Z",
     "iopub.status.idle": "2026-02-18T10:57:18.439680Z",
     "shell.execute_reply": "2026-02-18T10:57:18.439161Z"
    }
   },
   "outputs": [],
   "source": [
    "raco_events = raco_result.events\n",
    "\n",
    "if raco_events:\n",
    "    print(f\"Ra.co events ({len(raco_events)} total):\")\n",
    "    print(\"=\" * 70)\n",
    "    for i, event in enumerate(raco_events[:10]):\n",
    "        print(f\"\\n[{i+1}] {event.title}\")\n",
    "        print(f\"    City:    {event.location.city} | Venue: {event.location.venue_name}\")\n",
    "        print(f\"    Date:    {event.start_datetime}\")\n",
    "        print(f\"    Type:    {event.event_type}\")\n",
    "        print(f\"    Price:   {event.price.price_raw_text if event.price else 'N/A'}\")\n",
    "        print(f\"    Artists: {[a.name for a in event.artists]}\")\n",
    "        print(f\"    Source:  {event.source.source_url}\")\n",
    "        print(f\"    Quality: {event.data_quality_score:.2f}\")\n",
    "else:\n",
    "    print(\"No Ra.co events. Check pipeline logs above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combine-section",
   "metadata": {},
   "source": [
    "## Step 6: Combine Events from Both Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combine-events",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:18.441334Z",
     "iopub.status.busy": "2026-02-18T10:57:18.441209Z",
     "iopub.status.idle": "2026-02-18T10:57:18.444838Z",
     "shell.execute_reply": "2026-02-18T10:57:18.444339Z"
    }
   },
   "outputs": [],
   "source": [
    "all_events = raco_events + tm_events\n",
    "\n",
    "print(\"Combined Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Ra.co events:        {len(raco_events)}\")\n",
    "print(f\"Ticketmaster events: {len(tm_events)}\")\n",
    "print(f\"Total combined:      {len(all_events)}\")\n",
    "\n",
    "# Source breakdown\n",
    "from collections import Counter\n",
    "source_counts = Counter(e.source.source_name for e in all_events)\n",
    "print(f\"\\nBy source:\")\n",
    "for src, count in source_counts.most_common():\n",
    "    print(f\"  {src:20}: {count}\")\n",
    "\n",
    "# Event type breakdown\n",
    "type_counts = Counter(e.event_type for e in all_events)\n",
    "print(f\"\\nBy event type:\")\n",
    "for etype, count in type_counts.most_common():\n",
    "    print(f\"  {etype:20}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedup-section",
   "metadata": {},
   "source": [
    "## Step 7: Cross-Source Deduplication\n",
    "\n",
    "Apply `ExactMatchDeduplicator` across events from both sources. Events that appear in both Ticketmaster and Ra.co will be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:18.446098Z",
     "iopub.status.busy": "2026-02-18T10:57:18.446002Z",
     "iopub.status.idle": "2026-02-18T10:57:18.449218Z",
     "shell.execute_reply": "2026-02-18T10:57:18.448729Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.ingestion.deduplication import ExactMatchDeduplicator\n",
    "\n",
    "deduplicator = ExactMatchDeduplicator()\n",
    "deduplicated_events = deduplicator.deduplicate(all_events)\n",
    "\n",
    "print(\"Cross-Source Deduplication\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Events before dedup: {len(all_events)}\")\n",
    "print(f\"Events after dedup:  {len(deduplicated_events)}\")\n",
    "print(f\"Duplicates removed:  {len(all_events) - len(deduplicated_events)}\")\n",
    "\n",
    "if len(all_events) != len(deduplicated_events):\n",
    "    print(f\"\\nNote: {len(all_events) - len(deduplicated_events)} event(s) appeared in both sources\")\n",
    "else:\n",
    "    print(\"\\nNo cross-source duplicates found — sources cover distinct event sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df-section",
   "metadata": {},
   "source": [
    "## Step 8: Combined DataFrame\n",
    "\n",
    "Build a unified DataFrame with `source_name` for side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:18.450412Z",
     "iopub.status.busy": "2026-02-18T10:57:18.450333Z",
     "iopub.status.idle": "2026-02-18T10:57:18.825035Z",
     "shell.execute_reply": "2026-02-18T10:57:18.824604Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build dataframes per source then concat\n",
    "dfs = []\n",
    "if raco_events:\n",
    "    df_raco = ra_co.to_dataframe(raco_events)\n",
    "    dfs.append(df_raco)\n",
    "    print(f\"Ra.co DataFrame:        {df_raco.shape}\")\n",
    "\n",
    "if tm_events:\n",
    "    df_tm = ticketmaster.to_dataframe(tm_events)\n",
    "    dfs.append(df_tm)\n",
    "    print(f\"Ticketmaster DataFrame: {df_tm.shape}\")\n",
    "\n",
    "if dfs:\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nCombined DataFrame:     {df_combined.shape}\")\n",
    "else:\n",
    "    df_combined = pd.DataFrame()\n",
    "    print(\"No events to combine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:18.826014Z",
     "iopub.status.busy": "2026-02-18T10:57:18.825927Z",
     "iopub.status.idle": "2026-02-18T10:57:18.837241Z",
     "shell.execute_reply": "2026-02-18T10:57:18.836911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Side-by-side comparison view\n",
    "focus_cols = [\n",
    "    \"source_name\", \"title\", \"event_type\", \"city\",\n",
    "    \"venue_name\", \"start_datetime\",\n",
    "    \"price_minimum\", \"price_maximum\", \"price_currency\",\n",
    "    \"artists\", \"data_quality_score\",\n",
    "]\n",
    "available = [c for c in focus_cols if c in df_combined.columns]\n",
    "df_combined[available].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-section",
   "metadata": {},
   "source": [
    "## Step 9: Summary Statistics Per Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:18.838292Z",
     "iopub.status.busy": "2026-02-18T10:57:18.838239Z",
     "iopub.status.idle": "2026-02-18T10:57:18.850767Z",
     "shell.execute_reply": "2026-02-18T10:57:18.850377Z"
    }
   },
   "outputs": [],
   "source": [
    "if df_combined.empty:\n",
    "    print(\"No events to summarize.\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MULTI-SOURCE INGESTION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal events (combined): {len(df_combined)}\")\n",
    "\n",
    "    print(\"\\n--- Events per Source ---\")\n",
    "    print(df_combined.groupby(\"source_name\").size().to_string())\n",
    "\n",
    "    print(\"\\n--- Quality Score per Source ---\")\n",
    "    print(df_combined.groupby(\"source_name\")[\"data_quality_score\"].agg([\"mean\", \"min\", \"max\"]).round(3).to_string())\n",
    "\n",
    "    print(\"\\n--- Event Types per Source ---\")\n",
    "    print(df_combined.groupby([\"source_name\", \"event_type\"]).size().to_string())\n",
    "\n",
    "    print(\"\\n--- Artists Coverage per Source ---\")\n",
    "    artists_col = df_combined[\"artists\"].fillna(\"\")\n",
    "    has_artists = artists_col != \"\"\n",
    "    for src in df_combined[\"source_name\"].unique():\n",
    "        src_mask = df_combined[\"source_name\"] == src\n",
    "        total_src = src_mask.sum()\n",
    "        with_artists = (src_mask & has_artists).sum()\n",
    "        print(f\"  {src:20}: {with_artists}/{total_src} events have artists ({100*with_artists/total_src:.0f}%)\")\n",
    "\n",
    "    print(\"\\n--- Date Range per Source ---\")\n",
    "    for src in df_combined[\"source_name\"].unique():\n",
    "        src_df = df_combined[df_combined[\"source_name\"] == src]\n",
    "        print(f\"  {src}:\")\n",
    "        print(f\"    Earliest: {src_df['start_datetime'].min()}\")\n",
    "        print(f\"    Latest:   {src_df['start_datetime'].max()}\")\n",
    "\n",
    "    print(\"\\n--- Free vs Paid per Source ---\")\n",
    "    print(df_combined.groupby([\"source_name\", \"price_is_free\"]).size().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "field-coverage-section",
   "metadata": {},
   "source": [
    "## Step 10: City Stats & Full Field Coverage\n",
    "\n",
    "City-level ingestion breakdown per source, followed by a comprehensive field coverage table\n",
    "across all meaningful EventSchema sections (Core, Location, Pricing, Ticket, Organizer, etc.).\n",
    "\n",
    "- **✓** = 100% populated\n",
    "- **!** = 0% populated (gap to investigate)\n",
    "- *blank* = partial coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "field-coverage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:18.851856Z",
     "iopub.status.busy": "2026-02-18T10:57:18.851794Z",
     "iopub.status.idle": "2026-02-18T10:57:18.901735Z",
     "shell.execute_reply": "2026-02-18T10:57:18.901310Z"
    }
   },
   "outputs": [],
   "source": [
    "if not df_combined.empty:\n",
    "    # ── City Ingestion Statistics per Source ─────────────────────────────\n",
    "    print(\"City Ingestion Statistics per Source\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df_combined.groupby([\"source_name\", \"city\"]).size().rename(\"events\").to_string())\n",
    "\n",
    "    print()\n",
    "\n",
    "    # ── Field Coverage Table (all meaningful schema fields) ───────────────\n",
    "    # Grouped by schema section for readability\n",
    "    field_groups = {\n",
    "        \"Core\": [\n",
    "            \"title\", \"description\", \"event_type\", \"event_format\",\n",
    "            \"capacity\", \"age_restriction\", \"is_all_day\", \"is_recurring\",\n",
    "        ],\n",
    "        \"Timing\": [\n",
    "            \"start_datetime\", \"end_datetime\", \"duration_minutes\",\n",
    "        ],\n",
    "        \"Location\": [\n",
    "            \"venue_name\", \"street_address\", \"city\", \"state_or_region\",\n",
    "            \"postal_code\", \"country_code\", \"timezone\", \"latitude\", \"longitude\",\n",
    "        ],\n",
    "        \"Pricing\": [\n",
    "            \"price_is_free\", \"price_minimum\", \"price_maximum\",\n",
    "            \"price_currency\", \"price_raw_text\",\n",
    "        ],\n",
    "        \"Ticket\": [\n",
    "            \"ticket_url\", \"ticket_is_sold_out\", \"ticket_count_available\",\n",
    "        ],\n",
    "        \"Organizer\": [\n",
    "            \"organizer_name\", \"organizer_url\", \"organizer_phone\",\n",
    "            \"organizer_email\", \"organizer_follower_count\",\n",
    "        ],\n",
    "        \"Artists & Media\": [\n",
    "            \"artists\", \"media_assets_json\",\n",
    "        ],\n",
    "        \"Engagement\": [\n",
    "            \"engagement_going_count\", \"engagement_interested_count\",\n",
    "            \"engagement_views_count\", \"engagement_shares_count\",\n",
    "        ],\n",
    "        \"Source\": [\n",
    "            \"source_event_id\", \"source_url\", \"source_updated_at\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    sources = df_combined[\"source_name\"].unique()\n",
    "    col_w = 28\n",
    "\n",
    "    def field_pct(src_df, field):\n",
    "        \"\"\"Return non-null, non-empty percentage for a field.\"\"\"\n",
    "        if field not in src_df.columns:\n",
    "            return None\n",
    "        total = len(src_df)\n",
    "        if total == 0:\n",
    "            return None\n",
    "        col = src_df[field]\n",
    "        non_null = col.notna().sum()\n",
    "        if col.dtype == object:\n",
    "            non_null = (\n",
    "                col.notna()\n",
    "                & (col.astype(str) != \"\")\n",
    "                & (col.astype(str) != \"nan\")\n",
    "                & (col.astype(str) != \"None\")\n",
    "                & (col.astype(str) != \"[]\")\n",
    "            ).sum()\n",
    "        return 100 * non_null / total\n",
    "\n",
    "    print(\"Field Coverage by Source (% non-null / non-empty)\")\n",
    "    print(\"=\" * (col_w + 2 + 17 * len(sources)))\n",
    "    print(f\"  {'Field':<{col_w}}\", end=\"\")\n",
    "    for src in sources:\n",
    "        print(f\"  {src:>15}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "    for group_name, fields in field_groups.items():\n",
    "        print(f\"\\n  ── {group_name} {'─' * (col_w - len(group_name) - 4)}\")\n",
    "        for field in fields:\n",
    "            pcts = []\n",
    "            for src in sources:\n",
    "                src_df = df_combined[df_combined[\"source_name\"] == src]\n",
    "                pct = field_pct(src_df, field)\n",
    "                pcts.append(pct)\n",
    "\n",
    "            # Skip fields that are 0% for ALL sources (not interesting)\n",
    "            if all(p == 0 or p is None for p in pcts):\n",
    "                continue\n",
    "\n",
    "            print(f\"  {field:<{col_w}}\", end=\"\")\n",
    "            for pct in pcts:\n",
    "                if pct is None:\n",
    "                    print(f\"  {'N/A':>15}\", end=\"\")\n",
    "                else:\n",
    "                    marker = \" ✓\" if pct == 100 else (\" !\" if pct == 0 else \"  \")\n",
    "                    print(f\"  {pct:>13.0f}%{marker}\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-section",
   "metadata": {},
   "source": [
    "## Step 12: Save Pipeline Results (Parquet + Pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fzfjm5qrrpo",
   "metadata": {},
   "source": [
    "## Step 11: Persist to Database\n",
    "\n",
    "Write deduplicated events to PostgreSQL using `EventDataWriter`.\n",
    "Each event is persisted atomically (per-event rollback on failure).\n",
    "Run this after a successful pipeline execution to populate the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0mztdxiuee2j",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:18.902875Z",
     "iopub.status.busy": "2026-02-18T10:57:18.902805Z",
     "iopub.status.idle": "2026-02-18T10:57:19.118859Z",
     "shell.execute_reply": "2026-02-18T10:57:19.118475Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import psycopg2\n",
    "from src.ingestion.persist import EventDataWriter\n",
    "\n",
    "DATABASE_URL = os.environ.get(\"DATABASE_URL\", \"\")\n",
    "if not DATABASE_URL:\n",
    "    print(\"ERROR: DATABASE_URL not set — check .env\")\n",
    "else:\n",
    "    # Parse postgresql://user:pass@host:port/dbname\n",
    "    from urllib.parse import urlparse\n",
    "    u = urlparse(DATABASE_URL)\n",
    "    conn_params = dict(\n",
    "        host=u.hostname,\n",
    "        port=u.port or 5432,\n",
    "        dbname=u.path.lstrip(\"/\"),\n",
    "        user=u.username,\n",
    "        password=u.password,\n",
    "    )\n",
    "\n",
    "    events_to_persist = deduplicated_events  # from Step 7 cross-source dedup\n",
    "\n",
    "    print(f\"Persisting {len(events_to_persist)} events to PostgreSQL...\")\n",
    "    print(f\"  DB: {u.hostname}:{u.port}/{u.path.lstrip('/')}\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        writer = EventDataWriter(conn)\n",
    "\n",
    "        # Report taxonomy cache\n",
    "        print(f\"  Taxonomy cache: {len(writer._valid_primary_categories)} primary categories, \"\n",
    "              f\"{len(writer._valid_subcategories)} subcategories, \"\n",
    "              f\"{len(writer._valid_activities)} activities\")\n",
    "        print()\n",
    "\n",
    "        saved = writer.persist_batch(events_to_persist)\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"Persist complete: {saved}/{len(events_to_persist)} events saved\")\n",
    "        print(f\"  Failed/skipped: {len(events_to_persist) - saved}\")\n",
    "\n",
    "        # Quick verification query\n",
    "        conn2 = psycopg2.connect(**conn_params)\n",
    "        with conn2.cursor() as cur:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM events\")\n",
    "            total_events = cur.fetchone()[0]\n",
    "            cur.execute(\"SELECT COUNT(*) FROM locations\")\n",
    "            total_locations = cur.fetchone()[0]\n",
    "            cur.execute(\"SELECT COUNT(*) FROM sources\")\n",
    "            total_sources = cur.fetchone()[0]\n",
    "            cur.execute(\n",
    "                \"SELECT source_name, COUNT(*) FROM sources GROUP BY source_name ORDER BY source_name\"\n",
    "            )\n",
    "            by_source = cur.fetchall()\n",
    "        conn2.close()\n",
    "\n",
    "        print(f\"\\nDB verification:\")\n",
    "        print(f\"  events    : {total_events}\")\n",
    "        print(f\"  locations : {total_locations}\")\n",
    "        print(f\"  sources   : {total_sources}\")\n",
    "        print(f\"  By source :\")\n",
    "        for src_name, cnt in by_source:\n",
    "            print(f\"    {src_name:20}: {cnt}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:19.119818Z",
     "iopub.status.busy": "2026-02-18T10:57:19.119747Z",
     "iopub.status.idle": "2026-02-18T10:57:19.155822Z",
     "shell.execute_reply": "2026-02-18T10:57:19.155441Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output_dir = \"../data/raw\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if not df_combined.empty:\n",
    "    # Save combined parquet\n",
    "    parquet_path = f\"{output_dir}/multi_source_events.parquet\"\n",
    "    try:\n",
    "        df_combined.to_parquet(parquet_path, index=False, engine=\"pyarrow\")\n",
    "    except ImportError:\n",
    "        df_combined.to_parquet(parquet_path, index=False, engine=\"fastparquet\")\n",
    "    print(f\"Saved {len(df_combined)} events to {parquet_path}\")\n",
    "\n",
    "# Save pipeline results as pickle for downstream use\n",
    "results = {\"ra_co\": raco_result, \"ticketmaster\": tm_result}\n",
    "pkl_path = f\"{output_dir}/multi_source_results.pkl\"\n",
    "with open(pkl_path, \"wb\") as f:\n",
    "    pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Saved PipelineExecutionResult objects to {pkl_path}\")\n",
    "print(f\"  ra_co:        {raco_result.successful_events} events, status={raco_result.status.value}\")\n",
    "print(f\"  ticketmaster: {tm_result.successful_events} events, status={tm_result.status.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-section",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T10:57:19.156775Z",
     "iopub.status.busy": "2026-02-18T10:57:19.156709Z",
     "iopub.status.idle": "2026-02-18T10:57:19.159180Z",
     "shell.execute_reply": "2026-02-18T10:57:19.158867Z"
    }
   },
   "outputs": [],
   "source": [
    "await ra_co.close()\n",
    "await ticketmaster.close()\n",
    "print(\"Resources released.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
