{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Event Ingestion Pipeline Testing\n",
    "\n",
    "This notebook tests the **config-driven** event ingestion pipeline.\n",
    "All sources (Ra.co, Ticketmaster, etc.) are created through `PipelineFactory`\n",
    "using YAML configuration — no source-specific code needed.\n",
    "\n",
    "**Pipeline flow:**\n",
    "1. Factory reads `ingestion.yaml` and creates pipelines\n",
    "2. Each pipeline fetches raw data via its adapter (GraphQL / REST)\n",
    "3. FieldMapper extracts + transforms fields per config\n",
    "4. TaxonomyMapper assigns Human Experience Taxonomy dimensions\n",
    "5. Events are normalized to `EventSchema` and optionally enriched by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:18:20.161057Z",
     "iopub.status.busy": "2026-02-12T21:18:20.160894Z",
     "iopub.status.idle": "2026-02-12T21:18:20.168078Z",
     "shell.execute_reply": "2026-02-12T21:18:20.167506Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "# Setup path — point to services/api so src.* imports work\n",
    "API_ROOT = os.path.abspath(os.path.join(\"..\", \"services\", \"api\"))\n",
    "if API_ROOT not in sys.path:\n",
    "    sys.path.insert(0, API_ROOT)\n",
    "\n",
    "# Setup path — point to services/scrapping so scrapping.* imports work\n",
    "SCRAPPING_ROOT = os.path.abspath(os.path.join(\"..\", \"services\", \"scrapping\"))\n",
    "if SCRAPPING_ROOT not in sys.path:\n",
    "    sys.path.insert(0, SCRAPPING_ROOT)\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"API root: {API_ROOT}\")\n",
    "# print(f\"Scrapping root: {SCRAPPING_ROOT}\")\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "factory-section",
   "metadata": {},
   "source": [
    "## Step 1: PipelineFactory — List All Configured Sources\n",
    "\n",
    "The factory reads `ingestion.yaml` and can create pipelines for any enabled source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "factory-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:18:20.169681Z",
     "iopub.status.busy": "2026-02-12T21:18:20.169550Z",
     "iopub.status.idle": "2026-02-12T21:18:20.350264Z",
     "shell.execute_reply": "2026-02-12T21:18:20.349820Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.ingestion.factory import PipelineFactory\n",
    "\n",
    "factory = PipelineFactory()\n",
    "\n",
    "print(\"Configured Sources:\")\n",
    "print(\"=\" * 50)\n",
    "for name, info in factory.list_sources().items():\n",
    "    status = \"ENABLED\" if info[\"enabled\"] else \"disabled\"\n",
    "    print(f\"  {name:20} type={info['type']:10} [{status}]\")\n",
    "\n",
    "print(f\"\\nEnabled sources: {factory.list_enabled_sources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raco-section",
   "metadata": {},
   "source": [
    "## Step 2: Ra.co Pipeline — Multi-City Ingestion\n",
    "\n",
    "The Ra.co pipeline is created entirely from config. It uses:\n",
    "- GraphQL API adapter\n",
    "- Multi-city execution (Barcelona + Madrid via `defaults.areas`)\n",
    "- Date-window splitting for complete coverage\n",
    "- FieldMapper for extraction + transformations\n",
    "- FeatureExtractor (LLM) for taxonomy enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-raco",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:18:20.351428Z",
     "iopub.status.busy": "2026-02-12T21:18:20.351344Z",
     "iopub.status.idle": "2026-02-12T21:18:20.362858Z",
     "shell.execute_reply": "2026-02-12T21:18:20.362507Z"
    }
   },
   "outputs": [],
   "source": [
    "ra_co = factory.create_pipeline(\"ra_co\")\n",
    "\n",
    "print(f\"Pipeline: {ra_co.config.source_name}\")\n",
    "print(f\"Source type: {ra_co.source_type.value}\")\n",
    "print(f\"Protocol: {ra_co.source_config.protocol}\")\n",
    "print(f\"Endpoint: {ra_co.source_config.endpoint}\")\n",
    "print(f\"Areas: {ra_co.source_config.defaults.get('areas', {})}\")\n",
    "print(f\"Days ahead: {ra_co.source_config.defaults.get('days_ahead')}\")\n",
    "print(f\"Feature extractor: {ra_co.feature_extractor is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-raco",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:18:20.363879Z",
     "iopub.status.busy": "2026-02-12T21:18:20.363814Z",
     "iopub.status.idle": "2026-02-12T21:24:29.336851Z",
     "shell.execute_reply": "2026-02-12T21:24:29.336091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execute Ra.co pipeline (limited batch for faster notebook verification)\n",
    "# You can remove these limits for full runs.\n",
    "ra_co.source_config.defaults['areas'] = {'Barcelona': 20}\n",
    "raco_result = ra_co.execute(max_pages=1, page_size=5)\n",
    "\n",
    "print(\"Ra.co Pipeline Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Status: {raco_result.status.value}\")\n",
    "print(f\"Total raw events: {raco_result.total_events_processed}\")\n",
    "print(f\"Successful: {raco_result.successful_events}\")\n",
    "print(f\"Failed: {raco_result.failed_events}\")\n",
    "print(f\"Duration: {raco_result.duration_seconds:.2f}s\")\n",
    "print(f\"Success rate: {raco_result.success_rate:.1f}%\")\n",
    "print(f\"Cities: {raco_result.metadata.get('cities', [])}\")\n",
    "\n",
    "if raco_result.errors:\n",
    "    print(f\"\\nErrors: {raco_result.errors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d886a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.339716Z",
     "iopub.status.busy": "2026-02-12T21:24:29.339475Z",
     "iopub.status.idle": "2026-02-12T21:24:29.354876Z",
     "shell.execute_reply": "2026-02-12T21:24:29.354197Z"
    }
   },
   "outputs": [],
   "source": [
    "raco_result.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-raco-events",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.357270Z",
     "iopub.status.busy": "2026-02-12T21:24:29.357157Z",
     "iopub.status.idle": "2026-02-12T21:24:29.362141Z",
     "shell.execute_reply": "2026-02-12T21:24:29.361627Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show sample normalized events with ARTISTS focus\n",
    "if raco_result.events:\n",
    "    print(f\"Sample Events ({len(raco_result.events)} total):\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for i, event in enumerate(raco_result.events[:10]):\n",
    "        print(f\"\\n[{i+1}] {event.title}\")\n",
    "        print(f\"    City: {event.location.city} | Venue: {event.location.venue_name}\")\n",
    "        print(f\"    Date: {event.start_datetime}\")\n",
    "        print(f\"    Type: {event.event_type} | Price: {event.price.price_raw_text}\")\n",
    "        print(f\"    Artists: {[a.name for a in event.artists]}\")\n",
    "        print(f\"    Source URL: {event.source.source_url}\")\n",
    "        desc = (event.description or 'N/A')[:120]\n",
    "        print(f\"    Description: {desc}...\")\n",
    "        print(f\"    Quality: {event.data_quality_score:.2f}\")\n",
    "        print(f\"    Engagement: going={event.engagement.going_count if event.engagement else 'N/A'}, interested={event.engagement.interested_count if event.engagement else 'N/A'}\")\n",
    "        print(f\"    Custom fields: {event.custom_fields}\")\n",
    "else:\n",
    "    print(\"No events fetched. Check pipeline logs above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ticketmaster-section",
   "metadata": {},
   "source": [
    "## Step 3: Raw Event Inspection (pre-normalization)\n",
    "\n",
    "Inspect how FieldMapper extracts raw fields to understand the pipeline internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-ticketmaster",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.363730Z",
     "iopub.status.busy": "2026-02-12T21:24:29.363617Z",
     "iopub.status.idle": "2026-02-12T21:24:29.367208Z",
     "shell.execute_reply": "2026-02-12T21:24:29.366691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect raw parsed_event dict BEFORE normalization\n",
    "# This helps verify FieldMapper is extracting artists correctly\n",
    "print(\"=\" * 60)\n",
    "print(\"RAW FIELD MAPPER OUTPUT (parsed_event dict)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if hasattr(ra_co, '_last_raw_events') and ra_co._last_raw_events:\n",
    "    for i, raw in enumerate(ra_co._last_raw_events[:3]):\n",
    "        print(f\"\\n--- Raw event {i+1} ---\")\n",
    "        for key in ['title', 'artists', 'attending', 'interested_count',\n",
    "                     'flyer_front', 'pick_blurb', 'is_ticketed',\n",
    "                     'venue_name', 'minimum_age', 'venue_latitude', 'venue_longitude']:\n",
    "            print(f\"  {key}: {raw.get(key, 'N/A')}\")\n",
    "else:\n",
    "    print(\"No cached raw events — re-running field mapper on first page...\")\n",
    "    # Manually test the field mapper on a sample response\n",
    "    from src.ingestion.normalization.field_mapper import FieldMapper\n",
    "\n",
    "    mapper = FieldMapper(ra_co.source_config.field_mappings)\n",
    "    print(f\"  Configured field mappings: {list(ra_co.source_config.field_mappings.keys())}\")\n",
    "    print(f\"  Artists mapping: {ra_co.source_config.field_mappings.get('artists')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-section",
   "metadata": {},
   "source": [
    "## Step 5: Compressed HTML (raw_html) Inspection\n",
    "\n",
    "Verify that the `compressed_html` enrichment is working for RA.co events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cly0qq0s9ui",
   "metadata": {},
   "source": [
    "## Step 4: Artists Field Inspection\n",
    "\n",
    "Verify that `event.artists` is populated as `List[ArtistInfo]` (not stored in `custom_fields`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bnjs2mepe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.368961Z",
     "iopub.status.busy": "2026-02-12T21:24:29.368823Z",
     "iopub.status.idle": "2026-02-12T21:24:29.373720Z",
     "shell.execute_reply": "2026-02-12T21:24:29.373310Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deep inspection of the artists field\n",
    "events = raco_result.events\n",
    "\n",
    "# Count events with/without artists\n",
    "events_with_artists = [e for e in events if e.artists]\n",
    "events_without_artists = [e for e in events if not e.artists]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ARTISTS FIELD MAPPING INSPECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total events: {len(events)}\")\n",
    "print(f\"Events WITH artists: {len(events_with_artists)} ({100*len(events_with_artists)/len(events):.1f}%)\")\n",
    "print(f\"Events WITHOUT artists: {len(events_without_artists)} ({100*len(events_without_artists)/len(events):.1f}%)\")\n",
    "\n",
    "# Verify artists are ArtistInfo objects, not in custom_fields\n",
    "print(\"\\n--- Artist type check ---\")\n",
    "if events_with_artists:\n",
    "    sample = events_with_artists[0]\n",
    "    print(f\"  Type of event.artists: {type(sample.artists)}\")\n",
    "    print(f\"  Type of first artist: {type(sample.artists[0])}\")\n",
    "    print(f\"  First artist name: {sample.artists[0].name}\")\n",
    "    print(f\"  'artists' in custom_fields? {'artists' in (sample.custom_fields or {})}\")\n",
    "\n",
    "# Check custom_fields does NOT contain artists anymore\n",
    "print(\"\\n--- custom_fields check (should NOT contain 'artists' key) ---\")\n",
    "events_with_artists_in_cf = [\n",
    "    e for e in events\n",
    "    if e.custom_fields and \"artists\" in e.custom_fields\n",
    "]\n",
    "print(f\"Events with 'artists' in custom_fields: {events_with_artists_in_cf}\")\n",
    "\n",
    "# Show top events with most artists\n",
    "print(\"\\n--- Events with most artists ---\")\n",
    "sorted_by_artists = sorted(events, key=lambda e: len(e.artists), reverse=True)\n",
    "for e in sorted_by_artists[:10]:\n",
    "    names = [a.name for a in e.artists]\n",
    "    print(f\"  [{len(names)} artists] {e.title}: {names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.374933Z",
     "iopub.status.busy": "2026-02-12T21:24:29.374825Z",
     "iopub.status.idle": "2026-02-12T21:24:29.379051Z",
     "shell.execute_reply": "2026-02-12T21:24:29.378674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect compressed_html field on events\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPRESSED HTML (raw_html) INSPECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "events_with_html = [e for e in events if e.source.compressed_html]\n",
    "events_without_html = [e for e in events if not e.source.compressed_html]\n",
    "\n",
    "print(f\"Total events: {len(events)}\")\n",
    "print(f\"Events WITH compressed_html: {len(events_with_html)} ({100*len(events_with_html)/len(events):.1f}%)\")\n",
    "print(f\"Events WITHOUT compressed_html: {len(events_without_html)} ({100*len(events_without_html)/len(events):.1f}%)\")\n",
    "\n",
    "if events_with_html:\n",
    "    # Show sample compressed_html\n",
    "    sample = events_with_html[0]\n",
    "    html_text = sample.source.compressed_html\n",
    "    print(f\"\\n--- Sample compressed_html (first event with data) ---\")\n",
    "    print(f\"  Title: {sample.title}\")\n",
    "    print(f\"  Source URL: {sample.source.source_url}\")\n",
    "    print(f\"  HTML length: {len(html_text)} chars\")\n",
    "    print(f\"  First 500 chars: {html_text[:500]}...\")\n",
    "\n",
    "    # Stats\n",
    "    lengths = [len(e.source.compressed_html) for e in events_with_html]\n",
    "    avg_len = sum(lengths) / len(lengths)\n",
    "    print(f\"\\n--- Compressed HTML size stats ---\")\n",
    "    print(f\"  Min: {min(lengths)} chars\")\n",
    "    print(f\"  Max: {max(lengths)} chars\")\n",
    "    print(f\"  Avg: {avg_len:.0f} chars\")\n",
    "\n",
    "    # Content quality assessment\n",
    "    MIN_AVG_LEN = 200\n",
    "    short_events = [e for e in events_with_html if len(e.source.compressed_html) < MIN_AVG_LEN]\n",
    "    print(f\"\\n--- Content quality ---\")\n",
    "    print(f\"  Events with < {MIN_AVG_LEN} chars: {len(short_events)} / {len(events_with_html)}\")\n",
    "    if avg_len >= MIN_AVG_LEN:\n",
    "        print(f\"  Average content length ({avg_len:.0f}) >= {MIN_AVG_LEN}: GOOD\")\n",
    "    else:\n",
    "        print(f\"  WARNING: Average content length ({avg_len:.0f}) < {MIN_AVG_LEN}\")\n",
    "        print(f\"  This may indicate the scraping engine is not rendering JavaScript.\")\n",
    "        print(f\"  Consider using browser/hybrid engine for SPA sources.\")\n",
    "    if short_events:\n",
    "        print(f\"  Short content samples:\")\n",
    "        for e in short_events[:5]:\n",
    "            print(f\"    [{len(e.source.compressed_html)} chars] {e.title}: {e.source.compressed_html[:80]}...\")\n",
    "else:\n",
    "    print(\"\\nNO events have compressed_html!\")\n",
    "    print(\"Check that:\")\n",
    "    print(\"  1. 'scrapping' service is importable\")\n",
    "    print(\"  2. enrichment.compressed_html.enabled = true in ingestion.yaml\")\n",
    "    print(\"  3. RA.co pages are accessible with the configured engine\")\n",
    "    print(\"  4. Check pipeline logs for HTML enrichment errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-html-and-errors",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.380209Z",
     "iopub.status.busy": "2026-02-12T21:24:29.380135Z",
     "iopub.status.idle": "2026-02-12T21:24:29.383133Z",
     "shell.execute_reply": "2026-02-12T21:24:29.382686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Strict verification checks\n",
    "if not events:\n",
    "    raise AssertionError('No events ingested; cannot verify compressed_html coverage.')\n",
    "\n",
    "missing_html = [e.source.source_event_id for e in events if not (e.source.compressed_html or '').strip()]\n",
    "\n",
    "print(f'Total events checked: {len(events)}')\n",
    "print(f'Events missing compressed_html: {len(missing_html)}')\n",
    "\n",
    "if missing_html:\n",
    "    print('Sample missing compressed_html IDs:', missing_html[:10])\n",
    "\n",
    "assert not missing_html, 'Not all ingested events have compressed_html populated.'\n",
    "print('Verification passed: all events have compressed_html.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nxlpa86455j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization error severity breakdown\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALIZATION ERROR SEVERITY BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_errors = []\n",
    "for e in events:\n",
    "    all_errors.extend(e.normalization_errors)\n",
    "\n",
    "print(f\"Total normalization messages: {len(all_errors)}\")\n",
    "\n",
    "if all_errors:\n",
    "    # By severity\n",
    "    severity_counts = Counter(err.severity.value if hasattr(err.severity, 'value') else str(err.severity) for err in all_errors)\n",
    "    print(f\"\\n--- By Severity ---\")\n",
    "    for sev, count in severity_counts.most_common():\n",
    "        print(f\"  {sev:10}: {count}\")\n",
    "\n",
    "    # Show sample messages per severity\n",
    "    print(f\"\\n--- Sample Messages ---\")\n",
    "    for sev in severity_counts:\n",
    "        msgs = [err.message for err in all_errors if (err.severity.value if hasattr(err.severity, 'value') else str(err.severity)) == sev]\n",
    "        print(f\"  [{sev}] {msgs[0][:100]}\")\n",
    "else:\n",
    "    print(\"No normalization errors found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df-section",
   "metadata": {},
   "source": [
    "## Step 6: Deduplication\n",
    "\n",
    "Apply `ExactMatchDeduplicator` to the pipeline results and compare before/after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.384144Z",
     "iopub.status.busy": "2026-02-12T21:24:29.384075Z",
     "iopub.status.idle": "2026-02-12T21:24:29.387828Z",
     "shell.execute_reply": "2026-02-12T21:24:29.387440Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.ingestion.deduplication import ExactMatchDeduplicator, get_deduplicator, DeduplicationStrategy\n",
    "\n",
    "# Apply exact match deduplication\n",
    "deduplicator = ExactMatchDeduplicator()\n",
    "deduplicated_events = deduplicator.deduplicate(events)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEDUPLICATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Events before dedup: {len(events)}\")\n",
    "print(f\"Events after dedup:  {len(deduplicated_events)}\")\n",
    "print(f\"Duplicates removed:  {len(events) - len(deduplicated_events)}\")\n",
    "print(f\"Dedup ratio:         {100*(len(events) - len(deduplicated_events))/len(events):.1f}%\")\n",
    "\n",
    "# Show duplicates if any\n",
    "if len(events) != len(deduplicated_events):\n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    for event in events:\n",
    "        venue_name = event.location.venue_name or \"unknown_venue\"\n",
    "        key = (event.title, venue_name, str(event.start_datetime))\n",
    "        if key in seen:\n",
    "            duplicates.append(event)\n",
    "        else:\n",
    "            seen.add(key)\n",
    "\n",
    "    print(f\"\\n--- Duplicate events ---\")\n",
    "    for d in duplicates[:20]:\n",
    "        print(f\"  DUP: {d.title} @ {d.location.venue_name} ({d.start_datetime})\")\n",
    "else:\n",
    "    print(\"\\nNo duplicates found — all events are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-df-head",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.388841Z",
     "iopub.status.busy": "2026-02-12T21:24:29.388770Z",
     "iopub.status.idle": "2026-02-12T21:24:29.936729Z",
     "shell.execute_reply": "2026-02-12T21:24:29.936314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build DataFrame from deduplicated events\n",
    "df = ra_co.to_dataframe(deduplicated_events)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns ({len(df.columns)} total):\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# Show key fields for artists verification\n",
    "key_cols = [\"title\", \"artists\", \"custom_fields_json\", \"event_type\", \"data_quality_score\"]\n",
    "available = [c for c in key_cols if c in df.columns]\n",
    "df[available].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taxonomy-section",
   "metadata": {},
   "source": [
    "## Step 7: DataFrame Visualization & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taxonomy-table",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.937893Z",
     "iopub.status.busy": "2026-02-12T21:24:29.937803Z",
     "iopub.status.idle": "2026-02-12T21:24:29.944795Z",
     "shell.execute_reply": "2026-02-12T21:24:29.944366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show engagement and artist columns together\n",
    "focus_cols = [\n",
    "    \"title\", \"artists\", \"event_type\", \"city\",\n",
    "    \"engagement_going_count\", \"engagement_interested_count\",\n",
    "    \"age_restriction\", \"data_quality_score\",\n",
    "]\n",
    "available = [c for c in focus_cols if c in df.columns]\n",
    "df[available].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-section",
   "metadata": {},
   "source": [
    "## Step 7: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.945784Z",
     "iopub.status.busy": "2026-02-12T21:24:29.945716Z",
     "iopub.status.idle": "2026-02-12T21:24:29.955898Z",
     "shell.execute_reply": "2026-02-12T21:24:29.955541Z"
    }
   },
   "outputs": [],
   "source": [
    "if df.empty:\n",
    "    print(\"No events ingested — summary statistics unavailable.\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"INGESTION SUMMARY (after deduplication)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\nTotal events: {len(df)}\")\n",
    "    print(f\"Average quality score: {df['data_quality_score'].mean():.3f}\")\n",
    "\n",
    "    print(\"\\n--- By Source ---\")\n",
    "    print(df.groupby(\"source_name\").size().to_string())\n",
    "\n",
    "    print(\"\\n--- By City ---\")\n",
    "    print(df.groupby(\"city\").size().sort_values(ascending=False).to_string())\n",
    "\n",
    "    print(\"\\n--- By Event Type ---\")\n",
    "    print(df.groupby(\"event_type\").size().sort_values(ascending=False).to_string())\n",
    "\n",
    "    print(\"\\n--- Free vs Paid ---\")\n",
    "    print(df.groupby(\"price_is_free\").size().to_string())\n",
    "\n",
    "    # Artists stats\n",
    "    artists_col = df[\"artists\"].fillna(\"\")\n",
    "    events_with_artists_df = artists_col[artists_col != \"\"]\n",
    "    print(f\"\\n--- Artists ---\")\n",
    "    print(f\"Events with artist data: {len(events_with_artists_df)} / {len(df)}\")\n",
    "\n",
    "    print(\"\\n--- Date Range ---\")\n",
    "    print(f\"Earliest: {df['start_datetime'].min()}\")\n",
    "    print(f\"Latest:   {df['start_datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-section",
   "metadata": {},
   "source": [
    "## Step 8: Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-parquet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:29.956900Z",
     "iopub.status.busy": "2026-02-12T21:24:29.956844Z",
     "iopub.status.idle": "2026-02-12T21:24:30.046034Z",
     "shell.execute_reply": "2026-02-12T21:24:30.045627Z"
    }
   },
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    output_dir = \"../data/raw\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/events_all_sources.parquet\"\n",
    "    try:\n",
    "        df.to_parquet(output_path, index=False, engine='pyarrow')\n",
    "    except ImportError:\n",
    "        df.to_parquet(output_path, index=False, engine='fastparquet')\n",
    "    print(f\"Saved {len(df)} events to {output_path}\")\n",
    "else:\n",
    "    print(\"DataFrame is empty — skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vbrx9823e5j",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:30.047020Z",
     "iopub.status.busy": "2026-02-12T21:24:30.046957Z",
     "iopub.status.idle": "2026-02-12T21:24:30.050400Z",
     "shell.execute_reply": "2026-02-12T21:24:30.050105Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if raco_result:\n",
    "    output_dir = \"../data/raw\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pkl_path = f\"{output_dir}/raco_result.pkl\"\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(raco_result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"Saved PipelineExecutionResult to {pkl_path}\")\n",
    "    print(f\"  Events: {raco_result.successful_events}\")\n",
    "    print(f\"  Status: {raco_result.status.value}\")\n",
    "else:\n",
    "    print(\"No raco_result to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p9semftiwdo",
   "metadata": {},
   "source": [
    "## Step 9: Location Enrichment Verification\n",
    "\n",
    "Verify that `LocationParser` correctly populates `postal_code`, `state_or_region`, and `coordinates` on ingested events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xubza7xndm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location enrichment coverage analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"LOCATION ENRICHMENT VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "events = raco_result.events\n",
    "\n",
    "# Count fields\n",
    "has_coords = [e for e in events if e.location.coordinates is not None]\n",
    "has_postal = [e for e in events if e.location.postal_code is not None]\n",
    "has_state = [e for e in events if e.location.state_or_region is not None]\n",
    "has_address = [e for e in events if e.location.street_address is not None]\n",
    "\n",
    "total = len(events)\n",
    "print(f\"Total events: {total}\")\n",
    "print(f\"Events with street_address: {len(has_address)} ({100*len(has_address)/total:.1f}%)\")\n",
    "print(f\"Events with postal_code:    {len(has_postal)} ({100*len(has_postal)/total:.1f}%)\")\n",
    "print(f\"Events with state_or_region:{len(has_state)} ({100*len(has_state)/total:.1f}%)\")\n",
    "print(f\"Events with coordinates:    {len(has_coords)} ({100*len(has_coords)/total:.1f}%)\")\n",
    "\n",
    "# Show sample LocationInfo with enriched fields\n",
    "print(f\"\\n--- Sample Enriched Locations ---\")\n",
    "for e in events[:5]:\n",
    "    loc = e.location\n",
    "    coord_str = f\"({loc.coordinates.latitude}, {loc.coordinates.longitude})\" if loc.coordinates else \"None\"\n",
    "    print(f\"  {e.title[:50]}\")\n",
    "    print(f\"    venue: {loc.venue_name} | city: {loc.city} | country: {loc.country_code}\")\n",
    "    print(f\"    address: {loc.street_address}\")\n",
    "    print(f\"    postal_code: {loc.postal_code} | state: {loc.state_or_region}\")\n",
    "    print(f\"    coordinates: {coord_str}\")\n",
    "    print()\n",
    "\n",
    "# Assert postal_code coverage > 50% (only for events that have street_address)\n",
    "if has_address:\n",
    "    postal_rate = len(has_postal) / len(has_address)\n",
    "    print(f\"Postal code coverage (of events with address): {postal_rate:.1%}\")\n",
    "    assert postal_rate > 0.5, f\"Postal code coverage too low: {postal_rate:.1%} (expected > 50%)\"\n",
    "    print(\"PASS: postal_code coverage > 50%\")\n",
    "else:\n",
    "    print(\"WARNING: No events with street_address — cannot verify postal code extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-section",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T21:24:30.052017Z",
     "iopub.status.busy": "2026-02-12T21:24:30.051867Z",
     "iopub.status.idle": "2026-02-12T21:24:30.054318Z",
     "shell.execute_reply": "2026-02-12T21:24:30.053939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Close pipeline resources\n",
    "ra_co.close()\n",
    "print(\"Resources released.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
